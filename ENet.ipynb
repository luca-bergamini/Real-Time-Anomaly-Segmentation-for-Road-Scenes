{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Val2zA5UlIgK",
        "outputId": "94eb9046-a327-45bf-d0c9-dd5549536b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bo5LBH5uFvO",
        "outputId": "73abf74a-8579-4af5-992f-dde7344b5c49",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Real-Time-Anomaly-Segmentation-for-Road-Scenes'...\n",
            "remote: Enumerating objects: 983, done.\u001b[K\n",
            "remote: Counting objects: 100% (179/179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 983 (delta 130), reused 118 (delta 74), pack-reused 804 (from 1)\u001b[K\n",
            "Receiving objects: 100% (983/983), 386.41 MiB | 42.44 MiB/s, done.\n",
            "Resolving deltas: 100% (349/349), done.\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes\n",
            "Branch 'Luca-R' set up to track remote branch 'Luca-R' from 'origin'.\n",
            "Switched to a new branch 'Luca-R'\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!rm -rf Real-Time-Anomaly-Segmentation-for-Road-Scenes\n",
        "!git clone https://github.com/luca-bergamini/Real-Time-Anomaly-Segmentation-for-Road-Scenes.git\n",
        "%cd Real-Time-Anomaly-Segmentation-for-Road-Scenes\n",
        "!git checkout Luca-R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSKbtRwtuaQ-",
        "outputId": "c075f17c-d305-4001-baf2-0892f7420410",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: visdom in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from visdom) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom) (1.15.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom) (6.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from visdom) (3.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from visdom) (11.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2025.4.26)\n",
            "Requirement already satisfied: ood_metrics in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from ood_metrics) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.22 in /usr/local/lib/python3.11/dist-packages (from ood_metrics) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from ood_metrics) (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>=3.0->ood_metrics) (1.17.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=25be5861733b58c7a6238c5b074ffbe13660845ca226b6c5acd34305126b02e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=7c6a71a363d82754d734fd35aebf25e122ebed9adf75ecaef10f554c340f8668\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip3 install visdom\n",
        "!pip3 install ood_metrics\n",
        "!pip3 install torchvision\n",
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enet Training"
      ],
      "metadata": {
        "id": "mP-PQkZsfu1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd \"./train\" && python3 main.py \\\n",
        "--model enet \\\n",
        "--datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "--state \"../trained_models/enet_pretrained.pth\" \\\n",
        "--savedir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "--num-epochs=40 \\\n",
        "--decoder"
      ],
      "metadata": {
        "id": "U4YwEP3Xf4CF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd070b2-f6db-4256-bbbe-2da084bdb0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== DECODER TRAINING ===========\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/train\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val\n",
            "<class '__main__.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 3.067 (epoch: 1, step: 0) // Avg time/img: 0.1688 s\n",
            "loss: 2.713 (epoch: 1, step: 50) // Avg time/img: 0.0413 s\n",
            "loss: 2.41 (epoch: 1, step: 100) // Avg time/img: 0.0400 s\n",
            "loss: 2.188 (epoch: 1, step: 150) // Avg time/img: 0.0396 s\n",
            "loss: 2.026 (epoch: 1, step: 200) // Avg time/img: 0.0394 s\n",
            "loss: 1.91 (epoch: 1, step: 250) // Avg time/img: 0.0393 s\n",
            "loss: 1.81 (epoch: 1, step: 300) // Avg time/img: 0.0392 s\n",
            "loss: 1.74 (epoch: 1, step: 350) // Avg time/img: 0.0391 s\n",
            "loss: 1.677 (epoch: 1, step: 400) // Avg time/img: 0.0390 s\n",
            "loss: 1.625 (epoch: 1, step: 450) // Avg time/img: 0.0390 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/train/main.py:330: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/train/main.py:331: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  targets = Variable(labels, volatile=True)\n",
            "VAL loss: 0.8564 (epoch: 1, step: 0) // Avg time/img: 0.0157 s\n",
            "VAL loss: 1.168 (epoch: 1, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.93\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0004887358068751748\n",
            "loss: 0.9723 (epoch: 2, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 1.12 (epoch: 2, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 1.097 (epoch: 2, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 1.095 (epoch: 2, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 1.096 (epoch: 2, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 1.093 (epoch: 2, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 1.089 (epoch: 2, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 1.087 (epoch: 2, step: 350) // Avg time/img: 0.0387 s\n",
            "loss: 1.08 (epoch: 2, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 1.071 (epoch: 2, step: 450) // Avg time/img: 0.0387 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.7312 (epoch: 2, step: 0) // Avg time/img: 0.0150 s\n",
            "VAL loss: 1.032 (epoch: 2, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m20.99\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.00047744269081074987\n",
            "loss: 0.7197 (epoch: 3, step: 0) // Avg time/img: 0.0416 s\n",
            "loss: 1.016 (epoch: 3, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 1.001 (epoch: 3, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 1.002 (epoch: 3, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.9921 (epoch: 3, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.9856 (epoch: 3, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.9782 (epoch: 3, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.9673 (epoch: 3, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.9656 (epoch: 3, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.9608 (epoch: 3, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.664 (epoch: 3, step: 0) // Avg time/img: 0.0151 s\n",
            "VAL loss: 0.9887 (epoch: 3, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m25.23\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0004661198132697498\n",
            "loss: 0.7216 (epoch: 4, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.9588 (epoch: 4, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.9359 (epoch: 4, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.9252 (epoch: 4, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.911 (epoch: 4, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.9163 (epoch: 4, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.9086 (epoch: 4, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.9077 (epoch: 4, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.9016 (epoch: 4, step: 400) // Avg time/img: 0.0387 s\n",
            "loss: 0.8952 (epoch: 4, step: 450) // Avg time/img: 0.0387 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.6261 (epoch: 4, step: 0) // Avg time/img: 0.0171 s\n",
            "VAL loss: 0.9206 (epoch: 4, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.00045476628804148113\n",
            "loss: 0.8437 (epoch: 5, step: 0) // Avg time/img: 0.0398 s\n",
            "loss: 0.8707 (epoch: 5, step: 50) // Avg time/img: 0.0388 s\n",
            "loss: 0.8604 (epoch: 5, step: 100) // Avg time/img: 0.0388 s\n",
            "loss: 0.849 (epoch: 5, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 0.8461 (epoch: 5, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.8501 (epoch: 5, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.847 (epoch: 5, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.8438 (epoch: 5, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.8438 (epoch: 5, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.8451 (epoch: 5, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.5505 (epoch: 5, step: 0) // Avg time/img: 0.0151 s\n",
            "VAL loss: 0.8667 (epoch: 5, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.71\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.00044338117712860363\n",
            "loss: 0.8115 (epoch: 6, step: 0) // Avg time/img: 0.0402 s\n",
            "loss: 0.7823 (epoch: 6, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.8253 (epoch: 6, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.8305 (epoch: 6, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.8226 (epoch: 6, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.8209 (epoch: 6, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.823 (epoch: 6, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.8199 (epoch: 6, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.8139 (epoch: 6, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.8123 (epoch: 6, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.5922 (epoch: 6, step: 0) // Avg time/img: 0.0160 s\n",
            "VAL loss: 0.867 (epoch: 6, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.67\u001b[0m %\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.00043196348615140955\n",
            "loss: 0.7298 (epoch: 7, step: 0) // Avg time/img: 0.0404 s\n",
            "loss: 0.7918 (epoch: 7, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.7835 (epoch: 7, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.7782 (epoch: 7, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.7803 (epoch: 7, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.7776 (epoch: 7, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.7841 (epoch: 7, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.7854 (epoch: 7, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.7821 (epoch: 7, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.7779 (epoch: 7, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.5761 (epoch: 7, step: 0) // Avg time/img: 0.0150 s\n",
            "VAL loss: 0.8171 (epoch: 7, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.93\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.00042051215919672877\n",
            "loss: 0.8232 (epoch: 8, step: 0) // Avg time/img: 0.0411 s\n",
            "loss: 0.7617 (epoch: 8, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.7433 (epoch: 8, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.7428 (epoch: 8, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 0.7444 (epoch: 8, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.7477 (epoch: 8, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.7605 (epoch: 8, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.757 (epoch: 8, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.755 (epoch: 8, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.7552 (epoch: 8, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.5578 (epoch: 8, step: 0) // Avg time/img: 0.0156 s\n",
            "VAL loss: 0.7967 (epoch: 8, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.25\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.00040902607302542923\n",
            "loss: 0.6312 (epoch: 9, step: 0) // Avg time/img: 0.0400 s\n",
            "loss: 0.7543 (epoch: 9, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.7391 (epoch: 9, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.7368 (epoch: 9, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 0.7299 (epoch: 9, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.7261 (epoch: 9, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.7253 (epoch: 9, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.7212 (epoch: 9, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.725 (epoch: 9, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.7236 (epoch: 9, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.4993 (epoch: 9, step: 0) // Avg time/img: 0.0153 s\n",
            "VAL loss: 0.7608 (epoch: 9, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.23\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.000397504030536037\n",
            "loss: 0.7551 (epoch: 10, step: 0) // Avg time/img: 0.0406 s\n",
            "loss: 0.6927 (epoch: 10, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.7103 (epoch: 10, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.7101 (epoch: 10, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.7112 (epoch: 10, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.7123 (epoch: 10, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.7099 (epoch: 10, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.7082 (epoch: 10, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.7062 (epoch: 10, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.7057 (epoch: 10, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.5189 (epoch: 10, step: 0) // Avg time/img: 0.0158 s\n",
            "VAL loss: 0.7548 (epoch: 10, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.65\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.00038594475336178527\n",
            "loss: 0.8667 (epoch: 11, step: 0) // Avg time/img: 0.0415 s\n",
            "loss: 0.6992 (epoch: 11, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.6939 (epoch: 11, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.6912 (epoch: 11, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.6915 (epoch: 11, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.6907 (epoch: 11, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.6906 (epoch: 11, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.6894 (epoch: 11, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.6877 (epoch: 11, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.6841 (epoch: 11, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.5188 (epoch: 11, step: 0) // Avg time/img: 0.0151 s\n",
            "VAL loss: 0.7562 (epoch: 11, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.66\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.00037434687345337946\n",
            "loss: 0.5217 (epoch: 12, step: 0) // Avg time/img: 0.0396 s\n",
            "loss: 0.6432 (epoch: 12, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.6493 (epoch: 12, step: 100) // Avg time/img: 0.0388 s\n",
            "loss: 0.6525 (epoch: 12, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 0.6685 (epoch: 12, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.6709 (epoch: 12, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.6718 (epoch: 12, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.6682 (epoch: 12, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.6673 (epoch: 12, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.6649 (epoch: 12, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.5155 (epoch: 12, step: 0) // Avg time/img: 0.0170 s\n",
            "VAL loss: 0.719 (epoch: 12, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.16\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.00036270892346860996\n",
            "loss: 0.8497 (epoch: 13, step: 0) // Avg time/img: 0.0403 s\n",
            "loss: 0.6444 (epoch: 13, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.6464 (epoch: 13, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.6578 (epoch: 13, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.6541 (epoch: 13, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.6541 (epoch: 13, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.6569 (epoch: 13, step: 300) // Avg time/img: 0.0389 s\n",
            "loss: 0.6605 (epoch: 13, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.6576 (epoch: 13, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.6544 (epoch: 13, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.5553 (epoch: 13, step: 0) // Avg time/img: 0.0155 s\n",
            "VAL loss: 0.74 (epoch: 13, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.08\u001b[0m %\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.000351029325750848\n",
            "loss: 0.6134 (epoch: 14, step: 0) // Avg time/img: 0.0396 s\n",
            "loss: 0.633 (epoch: 14, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.6349 (epoch: 14, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.6431 (epoch: 14, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.6388 (epoch: 14, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.6431 (epoch: 14, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.6367 (epoch: 14, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.6374 (epoch: 14, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.6347 (epoch: 14, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.632 (epoch: 14, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.4862 (epoch: 14, step: 0) // Avg time/img: 0.0174 s\n",
            "VAL loss: 0.6805 (epoch: 14, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m36.36\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0003393063796290625\n",
            "loss: 0.6347 (epoch: 15, step: 0) // Avg time/img: 0.0404 s\n",
            "loss: 0.6285 (epoch: 15, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.6283 (epoch: 15, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.6285 (epoch: 15, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.6258 (epoch: 15, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.6183 (epoch: 15, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.6199 (epoch: 15, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.6205 (epoch: 15, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.6224 (epoch: 15, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.6241 (epoch: 15, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.4926 (epoch: 15, step: 0) // Avg time/img: 0.0153 s\n",
            "VAL loss: 0.6858 (epoch: 15, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m37.83\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0003275382467090493\n",
            "loss: 0.6142 (epoch: 16, step: 0) // Avg time/img: 0.0405 s\n",
            "loss: 0.568 (epoch: 16, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.5832 (epoch: 16, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5864 (epoch: 16, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 0.591 (epoch: 16, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5941 (epoch: 16, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.6001 (epoch: 16, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.6042 (epoch: 16, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.6114 (epoch: 16, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.6119 (epoch: 16, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.4968 (epoch: 16, step: 0) // Avg time/img: 0.0153 s\n",
            "VAL loss: 0.6612 (epoch: 16, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m38.92\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.00031572293374467766\n",
            "loss: 0.8191 (epoch: 17, step: 0) // Avg time/img: 0.0408 s\n",
            "loss: 0.5791 (epoch: 17, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5831 (epoch: 17, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5976 (epoch: 17, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5952 (epoch: 17, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5936 (epoch: 17, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5971 (epoch: 17, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5965 (epoch: 17, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5963 (epoch: 17, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5969 (epoch: 17, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.5121 (epoch: 17, step: 0) // Avg time/img: 0.0153 s\n",
            "VAL loss: 0.6614 (epoch: 17, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.28\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0003038582725730446\n",
            "loss: 0.4774 (epoch: 18, step: 0) // Avg time/img: 0.0398 s\n",
            "loss: 0.5521 (epoch: 18, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.5716 (epoch: 18, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5742 (epoch: 18, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.574 (epoch: 18, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5733 (epoch: 18, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5737 (epoch: 18, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5801 (epoch: 18, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5825 (epoch: 18, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5841 (epoch: 18, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.4625 (epoch: 18, step: 0) // Avg time/img: 0.0162 s\n",
            "VAL loss: 0.6409 (epoch: 18, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.00029194189645999016\n",
            "loss: 0.4966 (epoch: 19, step: 0) // Avg time/img: 0.0412 s\n",
            "loss: 0.5945 (epoch: 19, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5801 (epoch: 19, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5723 (epoch: 19, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5736 (epoch: 19, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5744 (epoch: 19, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5765 (epoch: 19, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5804 (epoch: 19, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.579 (epoch: 19, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5787 (epoch: 19, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.5068 (epoch: 19, step: 0) // Avg time/img: 0.0164 s\n",
            "VAL loss: 0.6445 (epoch: 19, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.81\u001b[0m %\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.00027997121202042266\n",
            "loss: 0.4356 (epoch: 20, step: 0) // Avg time/img: 0.0406 s\n",
            "loss: 0.5699 (epoch: 20, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5595 (epoch: 20, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5637 (epoch: 20, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5623 (epoch: 20, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.5715 (epoch: 20, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.5716 (epoch: 20, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5726 (epoch: 20, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5716 (epoch: 20, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5705 (epoch: 20, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.4671 (epoch: 20, step: 0) // Avg time/img: 0.0179 s\n",
            "VAL loss: 0.6297 (epoch: 20, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.99\u001b[0m %\n",
            "----- TRAINING - EPOCH 21 -----\n",
            "LEARNING RATE:  0.0002679433656340733\n",
            "loss: 0.4438 (epoch: 21, step: 0) // Avg time/img: 0.0427 s\n",
            "loss: 0.5436 (epoch: 21, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5499 (epoch: 21, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5546 (epoch: 21, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5622 (epoch: 21, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5639 (epoch: 21, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5661 (epoch: 21, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5642 (epoch: 21, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5615 (epoch: 21, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5604 (epoch: 21, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 21 -----\n",
            "VAL loss: 0.4601 (epoch: 21, step: 0) // Avg time/img: 0.0158 s\n",
            "VAL loss: 0.6196 (epoch: 21, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m41.15\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 21)\n",
            "----- TRAINING - EPOCH 22 -----\n",
            "LEARNING RATE:  0.0002558552029464411\n",
            "loss: 0.4397 (epoch: 22, step: 0) // Avg time/img: 0.0412 s\n",
            "loss: 0.5417 (epoch: 22, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.54 (epoch: 22, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5441 (epoch: 22, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.547 (epoch: 22, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5491 (epoch: 22, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5463 (epoch: 22, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5479 (epoch: 22, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5484 (epoch: 22, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5484 (epoch: 22, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 22 -----\n",
            "VAL loss: 0.4743 (epoch: 22, step: 0) // Avg time/img: 0.0160 s\n",
            "VAL loss: 0.6362 (epoch: 22, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m41.08\u001b[0m %\n",
            "----- TRAINING - EPOCH 23 -----\n",
            "LEARNING RATE:  0.00024370321958949772\n",
            "loss: 0.4536 (epoch: 23, step: 0) // Avg time/img: 0.0410 s\n",
            "loss: 0.5799 (epoch: 23, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.5549 (epoch: 23, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5435 (epoch: 23, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5429 (epoch: 23, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5393 (epoch: 23, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5384 (epoch: 23, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5378 (epoch: 23, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5408 (epoch: 23, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5448 (epoch: 23, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 23 -----\n",
            "VAL loss: 0.4393 (epoch: 23, step: 0) // Avg time/img: 0.0150 s\n",
            "VAL loss: 0.6138 (epoch: 23, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m42.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 23)\n",
            "----- TRAINING - EPOCH 24 -----\n",
            "LEARNING RATE:  0.0002314835006208722\n",
            "loss: 0.6993 (epoch: 24, step: 0) // Avg time/img: 0.0401 s\n",
            "loss: 0.5565 (epoch: 24, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5421 (epoch: 24, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.538 (epoch: 24, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.537 (epoch: 24, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.5349 (epoch: 24, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5344 (epoch: 24, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5367 (epoch: 24, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5374 (epoch: 24, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5369 (epoch: 24, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 24 -----\n",
            "VAL loss: 0.5565 (epoch: 24, step: 0) // Avg time/img: 0.0156 s\n",
            "VAL loss: 0.6599 (epoch: 24, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.14\u001b[0m %\n",
            "----- TRAINING - EPOCH 25 -----\n",
            "LEARNING RATE:  0.00021919164527704348\n",
            "loss: 0.4427 (epoch: 25, step: 0) // Avg time/img: 0.0400 s\n",
            "loss: 0.5267 (epoch: 25, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.5247 (epoch: 25, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5352 (epoch: 25, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5372 (epoch: 25, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5413 (epoch: 25, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.541 (epoch: 25, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.537 (epoch: 25, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.535 (epoch: 25, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5346 (epoch: 25, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 25 -----\n",
            "VAL loss: 0.5553 (epoch: 25, step: 0) // Avg time/img: 0.0150 s\n",
            "VAL loss: 0.6363 (epoch: 25, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m41.32\u001b[0m %\n",
            "----- TRAINING - EPOCH 26 -----\n",
            "LEARNING RATE:  0.0002068226723291381\n",
            "loss: 0.5503 (epoch: 26, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.5139 (epoch: 26, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.5233 (epoch: 26, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5211 (epoch: 26, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5186 (epoch: 26, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.5197 (epoch: 26, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.5202 (epoch: 26, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5171 (epoch: 26, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5201 (epoch: 26, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5202 (epoch: 26, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 26 -----\n",
            "VAL loss: 0.4212 (epoch: 26, step: 0) // Avg time/img: 0.0154 s\n",
            "VAL loss: 0.6122 (epoch: 26, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m42.37\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 26)\n",
            "----- TRAINING - EPOCH 27 -----\n",
            "LEARNING RATE:  0.00019437089939938174\n",
            "loss: 0.456 (epoch: 27, step: 0) // Avg time/img: 0.0406 s\n",
            "loss: 0.4959 (epoch: 27, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5046 (epoch: 27, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5134 (epoch: 27, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5182 (epoch: 27, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.5174 (epoch: 27, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.5167 (epoch: 27, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5179 (epoch: 27, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5221 (epoch: 27, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5217 (epoch: 27, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 27 -----\n",
            "VAL loss: 0.4256 (epoch: 27, step: 0) // Avg time/img: 0.0152 s\n",
            "VAL loss: 0.6248 (epoch: 27, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m41.98\u001b[0m %\n",
            "----- TRAINING - EPOCH 28 -----\n",
            "LEARNING RATE:  0.0001818297866778471\n",
            "loss: 0.5677 (epoch: 28, step: 0) // Avg time/img: 0.0399 s\n",
            "loss: 0.4984 (epoch: 28, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.4972 (epoch: 28, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5029 (epoch: 28, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5032 (epoch: 28, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.5039 (epoch: 28, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5071 (epoch: 28, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5071 (epoch: 28, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5099 (epoch: 28, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.511 (epoch: 28, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 28 -----\n",
            "VAL loss: 0.4132 (epoch: 28, step: 0) // Avg time/img: 0.0155 s\n",
            "VAL loss: 0.6058 (epoch: 28, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m43.44\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 28)\n",
            "----- TRAINING - EPOCH 29 -----\n",
            "LEARNING RATE:  0.00016919173095082495\n",
            "loss: 0.5118 (epoch: 29, step: 0) // Avg time/img: 0.0405 s\n",
            "loss: 0.5083 (epoch: 29, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.4963 (epoch: 29, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5036 (epoch: 29, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5021 (epoch: 29, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.506 (epoch: 29, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.5024 (epoch: 29, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.5 (epoch: 29, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.5004 (epoch: 29, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.5036 (epoch: 29, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 29 -----\n",
            "VAL loss: 0.4309 (epoch: 29, step: 0) // Avg time/img: 0.0157 s\n",
            "VAL loss: 0.6063 (epoch: 29, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m44.20\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 29)\n",
            "----- TRAINING - EPOCH 30 -----\n",
            "LEARNING RATE:  0.00015644778861416783\n",
            "loss: 0.469 (epoch: 30, step: 0) // Avg time/img: 0.0403 s\n",
            "loss: 0.4873 (epoch: 30, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.5052 (epoch: 30, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.4981 (epoch: 30, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.5033 (epoch: 30, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.5006 (epoch: 30, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.4971 (epoch: 30, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.498 (epoch: 30, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4972 (epoch: 30, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4983 (epoch: 30, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 30 -----\n",
            "VAL loss: 0.4245 (epoch: 30, step: 0) // Avg time/img: 0.0162 s\n",
            "VAL loss: 0.625 (epoch: 30, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m43.73\u001b[0m %\n",
            "----- TRAINING - EPOCH 31 -----\n",
            "LEARNING RATE:  0.00014358729437462936\n",
            "loss: 0.4085 (epoch: 31, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.4914 (epoch: 31, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5027 (epoch: 31, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.5009 (epoch: 31, step: 150) // Avg time/img: 0.0388 s\n",
            "loss: 0.4962 (epoch: 31, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.4946 (epoch: 31, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.4934 (epoch: 31, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4953 (epoch: 31, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4959 (epoch: 31, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.495 (epoch: 31, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 31 -----\n",
            "VAL loss: 0.4754 (epoch: 31, step: 0) // Avg time/img: 0.0165 s\n",
            "VAL loss: 0.6168 (epoch: 31, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m43.83\u001b[0m %\n",
            "----- TRAINING - EPOCH 32 -----\n",
            "LEARNING RATE:  0.00013059732174533928\n",
            "loss: 0.6137 (epoch: 32, step: 0) // Avg time/img: 0.0402 s\n",
            "loss: 0.5046 (epoch: 32, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.5011 (epoch: 32, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.4914 (epoch: 32, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4883 (epoch: 32, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4879 (epoch: 32, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.4901 (epoch: 32, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4934 (epoch: 32, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4922 (epoch: 32, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4909 (epoch: 32, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 32 -----\n",
            "VAL loss: 0.4469 (epoch: 32, step: 0) // Avg time/img: 0.0152 s\n",
            "VAL loss: 0.6048 (epoch: 32, step: 50) // Avg time/img: 0.0144 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m44.57\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 32)\n",
            "----- TRAINING - EPOCH 33 -----\n",
            "LEARNING RATE:  0.00011746189430880188\n",
            "loss: 0.4078 (epoch: 33, step: 0) // Avg time/img: 0.0405 s\n",
            "loss: 0.4807 (epoch: 33, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.4833 (epoch: 33, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.483 (epoch: 33, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4861 (epoch: 33, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4822 (epoch: 33, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.4829 (epoch: 33, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4841 (epoch: 33, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.488 (epoch: 33, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4865 (epoch: 33, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 33 -----\n",
            "VAL loss: 0.4057 (epoch: 33, step: 0) // Avg time/img: 0.0160 s\n",
            "VAL loss: 0.5828 (epoch: 33, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m45.16\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 33)\n",
            "----- TRAINING - EPOCH 34 -----\n",
            "LEARNING RATE:  0.00010416078593278445\n",
            "loss: 0.4872 (epoch: 34, step: 0) // Avg time/img: 0.0400 s\n",
            "loss: 0.481 (epoch: 34, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.4734 (epoch: 34, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.4762 (epoch: 34, step: 150) // Avg time/img: 0.0390 s\n",
            "loss: 0.4739 (epoch: 34, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4729 (epoch: 34, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.4764 (epoch: 34, step: 300) // Avg time/img: 0.0389 s\n",
            "loss: 0.4793 (epoch: 34, step: 350) // Avg time/img: 0.0389 s\n",
            "loss: 0.4789 (epoch: 34, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4772 (epoch: 34, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 34 -----\n",
            "VAL loss: 0.3732 (epoch: 34, step: 0) // Avg time/img: 0.0154 s\n",
            "VAL loss: 0.596 (epoch: 34, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m44.78\u001b[0m %\n",
            "----- TRAINING - EPOCH 35 -----\n",
            "LEARNING RATE:  9.066760365683728e-05\n",
            "loss: 0.4798 (epoch: 35, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.4737 (epoch: 35, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.4765 (epoch: 35, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.4731 (epoch: 35, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4774 (epoch: 35, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.4777 (epoch: 35, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.4764 (epoch: 35, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4761 (epoch: 35, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.475 (epoch: 35, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4772 (epoch: 35, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 35 -----\n",
            "VAL loss: 0.4101 (epoch: 35, step: 0) // Avg time/img: 0.0155 s\n",
            "VAL loss: 0.5937 (epoch: 35, step: 50) // Avg time/img: 0.0142 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m45.53\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 35)\n",
            "----- TRAINING - EPOCH 36 -----\n",
            "LEARNING RATE:  7.694652583405728e-05\n",
            "loss: 0.4105 (epoch: 36, step: 0) // Avg time/img: 0.0403 s\n",
            "loss: 0.4613 (epoch: 36, step: 50) // Avg time/img: 0.0389 s\n",
            "loss: 0.465 (epoch: 36, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.4658 (epoch: 36, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4668 (epoch: 36, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4698 (epoch: 36, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.4717 (epoch: 36, step: 300) // Avg time/img: 0.0389 s\n",
            "loss: 0.4702 (epoch: 36, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4697 (epoch: 36, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4712 (epoch: 36, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 36 -----\n",
            "VAL loss: 0.3673 (epoch: 36, step: 0) // Avg time/img: 0.0163 s\n",
            "VAL loss: 0.5924 (epoch: 36, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m45.93\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 36)\n",
            "----- TRAINING - EPOCH 37 -----\n",
            "LEARNING RATE:  6.294627058970836e-05\n",
            "loss: 0.6363 (epoch: 37, step: 0) // Avg time/img: 0.0411 s\n",
            "loss: 0.4853 (epoch: 37, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.4685 (epoch: 37, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.4705 (epoch: 37, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.469 (epoch: 37, step: 200) // Avg time/img: 0.0388 s\n",
            "loss: 0.4694 (epoch: 37, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.4697 (epoch: 37, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4691 (epoch: 37, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4691 (epoch: 37, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4667 (epoch: 37, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 37 -----\n",
            "VAL loss: 0.4033 (epoch: 37, step: 0) // Avg time/img: 0.0153 s\n",
            "VAL loss: 0.5864 (epoch: 37, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m46.09\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 37)\n",
            "----- TRAINING - EPOCH 38 -----\n",
            "LEARNING RATE:  4.858756575557835e-05\n",
            "loss: 0.3254 (epoch: 38, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.4704 (epoch: 38, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.4646 (epoch: 38, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.4636 (epoch: 38, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4615 (epoch: 38, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4573 (epoch: 38, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.4612 (epoch: 38, step: 300) // Avg time/img: 0.0389 s\n",
            "loss: 0.4635 (epoch: 38, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.464 (epoch: 38, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4625 (epoch: 38, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 38 -----\n",
            "VAL loss: 0.4173 (epoch: 38, step: 0) // Avg time/img: 0.0164 s\n",
            "VAL loss: 0.6014 (epoch: 38, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m45.99\u001b[0m %\n",
            "----- TRAINING - EPOCH 39 -----\n",
            "LEARNING RATE:  3.373207119183911e-05\n",
            "loss: 0.4698 (epoch: 39, step: 0) // Avg time/img: 0.0410 s\n",
            "loss: 0.4692 (epoch: 39, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.4627 (epoch: 39, step: 100) // Avg time/img: 0.0389 s\n",
            "loss: 0.4629 (epoch: 39, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4648 (epoch: 39, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4609 (epoch: 39, step: 250) // Avg time/img: 0.0388 s\n",
            "loss: 0.4618 (epoch: 39, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4602 (epoch: 39, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4607 (epoch: 39, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4605 (epoch: 39, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 39 -----\n",
            "VAL loss: 0.4202 (epoch: 39, step: 0) // Avg time/img: 0.0162 s\n",
            "VAL loss: 0.601 (epoch: 39, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m46.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 39)\n",
            "----- TRAINING - EPOCH 40 -----\n",
            "LEARNING RATE:  1.8076569369899076e-05\n",
            "loss: 0.4941 (epoch: 40, step: 0) // Avg time/img: 0.0440 s\n",
            "loss: 0.4488 (epoch: 40, step: 50) // Avg time/img: 0.0390 s\n",
            "loss: 0.4583 (epoch: 40, step: 100) // Avg time/img: 0.0390 s\n",
            "loss: 0.4562 (epoch: 40, step: 150) // Avg time/img: 0.0389 s\n",
            "loss: 0.4569 (epoch: 40, step: 200) // Avg time/img: 0.0389 s\n",
            "loss: 0.4596 (epoch: 40, step: 250) // Avg time/img: 0.0389 s\n",
            "loss: 0.4542 (epoch: 40, step: 300) // Avg time/img: 0.0388 s\n",
            "loss: 0.4553 (epoch: 40, step: 350) // Avg time/img: 0.0388 s\n",
            "loss: 0.4571 (epoch: 40, step: 400) // Avg time/img: 0.0388 s\n",
            "loss: 0.4551 (epoch: 40, step: 450) // Avg time/img: 0.0388 s\n",
            "----- VALIDATING - EPOCH 40 -----\n",
            "VAL loss: 0.3916 (epoch: 40, step: 0) // Avg time/img: 0.0163 s\n",
            "VAL loss: 0.5915 (epoch: 40, step: 50) // Avg time/img: 0.0143 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m46.27\u001b[0m %\n",
            "Saving model as best\n",
            "save: /content/drive/MyDrive/training_results/enet//model_best.pth (epoch: 40)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVAL_IOU"
      ],
      "metadata": {
        "id": "8ZNGt_LW7O4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./eval/eval_iou.py \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "    --loadWeights \"model_best.pth\" \\\n",
        "    --loadModel \"./train/enet.py\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4 \\\n",
        "    --void"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jqCIyFh7TR4",
        "outputId": "10c4dffe-f666-4f72-9c27-d03c420df8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "---------------------------------------\n",
            "Took  189.5460171699524 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m90.05\u001b[0m Road\n",
            "\u001b[0m62.12\u001b[0m sidewalk\n",
            "\u001b[0m80.76\u001b[0m building\n",
            "\u001b[0m30.46\u001b[0m wall\n",
            "\u001b[0m25.89\u001b[0m fence\n",
            "\u001b[0m36.77\u001b[0m pole\n",
            "\u001b[0m21.91\u001b[0m traffic light\n",
            "\u001b[0m40.60\u001b[0m traffic sign\n",
            "\u001b[0m85.62\u001b[0m vegetation\n",
            "\u001b[0m47.47\u001b[0m terrain\n",
            "\u001b[0m87.94\u001b[0m sky\n",
            "\u001b[0m47.84\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m83.70\u001b[0m car\n",
            "\u001b[0m11.73\u001b[0m truck\n",
            "\u001b[0m29.37\u001b[0m bus\n",
            "\u001b[0m6.49\u001b[0m train\n",
            "\u001b[0m0.16\u001b[0m motorcycle\n",
            "\u001b[0m47.22\u001b[0m bicycle\n",
            "\u001b[0m60.63\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m44.84\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Void Class"
      ],
      "metadata": {
        "id": "LRwM6UEcBkBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for dataset in ['RoadAnomaly21','RoadObsticle21','fs_static','FS_LostFound_full','RoadAnomaly']:\n",
        "  for method in ['Void']:\n",
        "\n",
        "    format_file = os.listdir(f'/content/drive/MyDrive/Validation_Dataset/{dataset}/images')[0].split(\".\")[1]\n",
        "    input = f'/content/drive/MyDrive/Validation_Dataset/{dataset}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset} - method : {method}\")\n",
        "\n",
        "    !python3 ./eval/evalAnomaly.py \\\n",
        "    --input {input} \\\n",
        "    --loadModel './train/enet.py' \\\n",
        "    --method {method} \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "    --loadWeights 'model_best.pth'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s8HTSwPCQld",
        "outputId": "5814cc7f-9dc2-493a-e475-75f1e2430600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: RoadAnomaly21 - method : Void\n",
            "AUPRC score: 14.18350845440059\n",
            "FPR@TPR95: 85.49306421679567\n",
            "\n",
            "Dataset: RoadObsticle21 - method : Void\n",
            "AUPRC score: 1.4896421847847223\n",
            "FPR@TPR95: 51.67289908451357\n",
            "\n",
            "Dataset: fs_static - method : Void\n",
            "AUPRC score: 5.167231726281392\n",
            "FPR@TPR95: 43.00049303188575\n",
            "\n",
            "Dataset: FS_LostFound_full - method : Void\n",
            "AUPRC score: 3.7129541153773613\n",
            "FPR@TPR95: 75.10682335405883\n",
            "\n",
            "Dataset: RoadAnomaly - method : Void\n",
            "AUPRC score: 8.317108475052331\n",
            "FPR@TPR95: 79.48221203159046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ],
      "metadata": {
        "id": "QpBRkLFMjYs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pruning in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    print(f\"\\nRunning evaluation with pruning={pruning}\")\n",
        "    !python3 ./eval/eval_iou.py \\\n",
        "        --loadDir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "        --loadWeights \"model_best.pth\" \\\n",
        "        --subset val \\\n",
        "        --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "        --batch-size 1 \\\n",
        "        --num-workers 4 \\\n",
        "        --void \\\n",
        "        --pruning {pruning}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD6QIeR_jc22",
        "outputId": "2674ac46-0ef8-46ce-82a2-8b929f43784e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running evaluation with pruning=0.1\n",
            "Applying unstructured L1 pruning with 10.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 318284\n",
            "Pruned percentage: 9.53%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 88 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 3.96 GFLOPs\n",
            "Estimated time: 0.006169 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "100% 500/500 [01:59<00:00,  4.17it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0207 seconds\n",
            "Total inference time (model only): 10.33 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m24.49\u001b[0m Road\n",
            "\u001b[0m13.72\u001b[0m sidewalk\n",
            "\u001b[0m34.29\u001b[0m building\n",
            "\u001b[0m2.27\u001b[0m wall\n",
            "\u001b[0m0.27\u001b[0m fence\n",
            "\u001b[0m3.51\u001b[0m pole\n",
            "\u001b[0m0.34\u001b[0m traffic light\n",
            "\u001b[0m0.66\u001b[0m traffic sign\n",
            "\u001b[0m14.06\u001b[0m vegetation\n",
            "\u001b[0m0.24\u001b[0m terrain\n",
            "\u001b[0m0.39\u001b[0m sky\n",
            "\u001b[0m14.24\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m16.81\u001b[0m car\n",
            "\u001b[0m1.23\u001b[0m truck\n",
            "\u001b[0m0.80\u001b[0m bus\n",
            "\u001b[0m0.00\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m2.01\u001b[0m bicycle\n",
            "\u001b[0m28.28\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m7.88\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.2\n",
            "Applying unstructured L1 pruning with 20.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 284788\n",
            "Pruned percentage: 19.05%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 88 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 3.54 GFLOPs\n",
            "Estimated time: 0.005520 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "100% 500/500 [00:31<00:00, 15.99it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0210 seconds\n",
            "Total inference time (model only): 10.48 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m19.53\u001b[0m Road\n",
            "\u001b[0m11.68\u001b[0m sidewalk\n",
            "\u001b[0m30.86\u001b[0m building\n",
            "\u001b[0m1.17\u001b[0m wall\n",
            "\u001b[0m0.23\u001b[0m fence\n",
            "\u001b[0m3.94\u001b[0m pole\n",
            "\u001b[0m0.30\u001b[0m traffic light\n",
            "\u001b[0m0.28\u001b[0m traffic sign\n",
            "\u001b[0m14.28\u001b[0m vegetation\n",
            "\u001b[0m0.28\u001b[0m terrain\n",
            "\u001b[0m0.36\u001b[0m sky\n",
            "\u001b[0m14.35\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m13.38\u001b[0m car\n",
            "\u001b[0m1.04\u001b[0m truck\n",
            "\u001b[0m0.66\u001b[0m bus\n",
            "\u001b[0m0.00\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m1.37\u001b[0m bicycle\n",
            "\u001b[0m29.73\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m7.17\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.3\n",
            "Applying unstructured L1 pruning with 30.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 251269\n",
            "Pruned percentage: 28.58%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 88 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 3.12 GFLOPs\n",
            "Estimated time: 0.004870 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "100% 500/500 [00:31<00:00, 15.76it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0206 seconds\n",
            "Total inference time (model only): 10.29 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m6.55\u001b[0m Road\n",
            "\u001b[0m3.91\u001b[0m sidewalk\n",
            "\u001b[0m26.61\u001b[0m building\n",
            "\u001b[0m0.63\u001b[0m wall\n",
            "\u001b[0m0.33\u001b[0m fence\n",
            "\u001b[0m1.17\u001b[0m pole\n",
            "\u001b[0m0.13\u001b[0m traffic light\n",
            "\u001b[0m0.06\u001b[0m traffic sign\n",
            "\u001b[0m16.32\u001b[0m vegetation\n",
            "\u001b[0m0.02\u001b[0m terrain\n",
            "\u001b[0m0.14\u001b[0m sky\n",
            "\u001b[0m10.64\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m11.43\u001b[0m car\n",
            "\u001b[0m1.58\u001b[0m truck\n",
            "\u001b[0m0.65\u001b[0m bus\n",
            "\u001b[0m0.04\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m1.13\u001b[0m bicycle\n",
            "\u001b[0m29.44\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m5.54\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.4\n",
            "Applying unstructured L1 pruning with 40.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 217773\n",
            "Pruned percentage: 38.10%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 88 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 2.71 GFLOPs\n",
            "Estimated time: 0.004221 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "100% 500/500 [00:31<00:00, 16.12it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0206 seconds\n",
            "Total inference time (model only): 10.32 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m18.16\u001b[0m Road\n",
            "\u001b[0m4.37\u001b[0m sidewalk\n",
            "\u001b[0m44.51\u001b[0m building\n",
            "\u001b[0m0.07\u001b[0m wall\n",
            "\u001b[0m0.17\u001b[0m fence\n",
            "\u001b[0m2.76\u001b[0m pole\n",
            "\u001b[0m0.00\u001b[0m traffic light\n",
            "\u001b[0m0.18\u001b[0m traffic sign\n",
            "\u001b[0m16.37\u001b[0m vegetation\n",
            "\u001b[0m0.48\u001b[0m terrain\n",
            "\u001b[0m0.00\u001b[0m sky\n",
            "\u001b[0m11.90\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m13.50\u001b[0m car\n",
            "\u001b[0m0.80\u001b[0m truck\n",
            "\u001b[0m0.79\u001b[0m bus\n",
            "\u001b[0m0.03\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m0.27\u001b[0m bicycle\n",
            "\u001b[0m36.52\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m7.54\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.5\n",
            "Applying unstructured L1 pruning with 50.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 184252\n",
            "Pruned percentage: 47.63%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add_ encountered 88 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 2.29 GFLOPs\n",
            "Estimated time: 0.003571 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "100% 500/500 [00:31<00:00, 15.66it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0205 seconds\n",
            "Total inference time (model only): 10.25 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m4.69\u001b[0m Road\n",
            "\u001b[0m0.00\u001b[0m sidewalk\n",
            "\u001b[0m35.70\u001b[0m building\n",
            "\u001b[0m0.00\u001b[0m wall\n",
            "\u001b[0m0.03\u001b[0m fence\n",
            "\u001b[0m0.14\u001b[0m pole\n",
            "\u001b[0m0.00\u001b[0m traffic light\n",
            "\u001b[0m0.00\u001b[0m traffic sign\n",
            "\u001b[0m0.46\u001b[0m vegetation\n",
            "\u001b[0m0.00\u001b[0m terrain\n",
            "\u001b[0m0.00\u001b[0m sky\n",
            "\u001b[0m8.10\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m12.55\u001b[0m car\n",
            "\u001b[0m0.43\u001b[0m truck\n",
            "\u001b[0m0.41\u001b[0m bus\n",
            "\u001b[0m0.02\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m0.00\u001b[0m bicycle\n",
            "\u001b[0m28.66\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m4.56\u001b[0m %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "mP-PQkZsfu1V"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4Y3RrSeWcvaBc7lmzUjHQ"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}