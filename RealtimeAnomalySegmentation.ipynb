{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uHizf1B4W0Kw"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading"
      ],
      "metadata": {
        "id": "7Lt4A8JOpNDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md3zzTLxXicA",
        "outputId": "276020a2-3dd2-455d-dc5d-10898f3553d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf Real-Time-Anomaly-Segmentation-for-Road-Scenes\n",
        "!git clone https://github.com/luca-bergamini/Real-Time-Anomaly-Segmentation-for-Road-Scenes.git\n",
        "%cd Real-Time-Anomaly-Segmentation-for-Road-Scenes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55xtM5cZyRJ",
        "outputId": "c5031130-c4c6-406e-e2fc-4c6396b749ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Real-Time-Anomaly-Segmentation-for-Road-Scenes'...\n",
            "remote: Enumerating objects: 1224, done.\u001b[K\n",
            "remote: Counting objects: 100% (331/331), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 1224 (delta 292), reused 278 (delta 247), pack-reused 893 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1224/1224), 386.45 MiB | 44.86 MiB/s, done.\n",
            "Resolving deltas: 100% (534/534), done.\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install ood_metrics\n",
        "!pip3 install visdom\n",
        "!pip install fvcore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkdptlNXra2",
        "outputId": "0ef5db4a-81df-4051-e1a8-0f5917783f2d",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ood_metrics\n",
            "  Downloading ood_metrics-1.1.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from ood_metrics) (3.10.0)\n",
            "Collecting numpy<2.0,>=1.22 (from ood_metrics)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from ood_metrics) (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0,>=3.0->ood_metrics) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0,>=1.0->ood_metrics) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>=3.0->ood_metrics) (1.17.0)\n",
            "Downloading ood_metrics-1.1.2-py3-none-any.whl (6.1 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, ood_metrics\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 ood_metrics-1.1.2\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.11/dist-packages (from visdom) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom) (1.15.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom) (6.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from visdom) (3.4.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from visdom) (11.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom) (2025.4.26)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=3e5951ab109c981d9934c4a73020e7c6e4723318187309213997fcdf1507b22f\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\n",
            "Successfully built visdom\n",
            "Installing collected packages: visdom\n",
            "Successfully installed visdom-0.2.4\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.13.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=7d75f5954e0f0754af503407c88215db7ba94d4082ed43d4d3d16829c6a3c823\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=21c54bfd218369399a56256cd140bc6cebcc16d4dff25b8ea0094943a39d0770\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_paths = {\n",
        "    \"RoadAnomaly21\": \"/content/drive/MyDrive/Validation_Dataset/RoadAnomaly21/images/*.png\",\n",
        "    \"RoadObsticle21\": \"/content/drive/MyDrive/Validation_Dataset/RoadObsticle21/images/*.webp\",\n",
        "    \"fs_static\": \"/content/drive/MyDrive/Validation_Dataset/fs_static/images/*.jpg\",\n",
        "    \"FS_LostFound_full\": \"/content/drive/MyDrive/Validation_Dataset/FS_LostFound_full/images/*.png\",\n",
        "    \"RoadAnomaly\": \"/content/drive/MyDrive/Validation_Dataset/RoadAnomaly/images/*.jpg\"\n",
        "}"
      ],
      "metadata": {
        "id": "JJH9KuwTju0x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ERF-Net Inference"
      ],
      "metadata": {
        "id": "ktpUXvC-q_39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval/evalAnomaly.py \\\n",
        "  --input \"/content/drive/MyDrive/Validation_Dataset/RoadAnomaly21/images/*.png\" \\\n",
        "  --loadDir \"./trained_models/\" \\\n",
        "  --loadWeights \"erfnet_pretrained.pth\" \\\n",
        "  --loadModel \"erfnet.py\" \\\n",
        "  --subset val \\\n",
        "  --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "  --batch-size 1 \\\n",
        "  --num-workers 4 \\\n",
        "  --method \"MSP\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqJXnbara724",
        "outputId": "d45297ce-3b14-4b52-ccbd-48af55e6eed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUPRC score: 29.095121622764324\n",
            "FPR@TPR95: 62.548789859326625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\n Evaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        print(f\"Method: {method}\")\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"./trained_models/\" \\\n",
        "            --loadWeights \"erfnet_pretrained.pth\" \\\n",
        "            --loadModel \"erfnet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 4 \\\n",
        "            --method \"{method}\"\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "TCrMEpW9_-Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5ac7c0-b2e0-4977-b5a9-0714ceb799cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "Method: MSP\n",
            "AUPRC score: 29.095121622764324\n",
            "FPR@TPR95: 62.548789859326625\n",
            "\n",
            "\n",
            "Method: MaxLogit\n",
            "AUPRC score: 38.31957797222208\n",
            "FPR@TPR95: 59.3370558914899\n",
            "\n",
            "\n",
            "Method: MaxEntropy\n",
            "AUPRC score: 30.968609042248207\n",
            "FPR@TPR95: 62.65845671952056\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "Method: MSP\n",
            "AUPRC score: 2.7099746270831964\n",
            "FPR@TPR95: 65.22309081830707\n",
            "\n",
            "\n",
            "Method: MaxLogit\n",
            "AUPRC score: 4.626567617520253\n",
            "FPR@TPR95: 48.443439151949555\n",
            "\n",
            "\n",
            "Method: MaxEntropy\n",
            "AUPRC score: 3.044403505658315\n",
            "FPR@TPR95: 65.91177264537156\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "Method: MSP\n",
            "AUPRC score: 7.472042122551045\n",
            "FPR@TPR95: 41.83689056458776\n",
            "\n",
            "\n",
            "Method: MaxLogit\n",
            "AUPRC score: 9.498677970785756\n",
            "FPR@TPR95: 40.3000747567442\n",
            "\n",
            "\n",
            "Method: MaxEntropy\n",
            "AUPRC score: 8.83648474113272\n",
            "FPR@TPR95: 41.54595598789666\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "Method: MSP\n",
            "AUPRC score: 1.7490388138046746\n",
            "FPR@TPR95: 50.59403252456551\n",
            "\n",
            "\n",
            "Method: MaxLogit\n",
            "AUPRC score: 3.3014401015087245\n",
            "FPR@TPR95: 45.494876929038305\n",
            "\n",
            "\n",
            "Method: MaxEntropy\n",
            "AUPRC score: 2.5839560389480267\n",
            "FPR@TPR95: 50.162803536071365\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "Method: MSP\n",
            "AUPRC score: 12.42265113945068\n",
            "FPR@TPR95: 82.57546412366915\n",
            "\n",
            "\n",
            "Method: MaxLogit\n",
            "AUPRC score: 15.581983301641019\n",
            "FPR@TPR95: 73.24766535735604\n",
            "\n",
            "\n",
            "Method: MaxEntropy\n",
            "AUPRC score: 12.668527775557207\n",
            "FPR@TPR95: 82.74860305094576\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 eval/eval_iou.py \\\n",
        "    --loadDir \"./trained_models/\" \\\n",
        "    --loadWeights \"erfnet_pretrained.pth\" \\\n",
        "    --loadModel \"erfnet.py\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1D2NAlsiGS3",
        "outputId": "e15ae457-0402-45d3-cd9d-b6325b2fb00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 2066835\n",
            "Pruned percentage: 0.00%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 26.93 GFLOPs\n",
            "Estimated time: 0.042004 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:29<00:00,  5.61it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0430 seconds\n",
            "Total inference time (model only): 21.48 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ERF-Net Temperature Inference"
      ],
      "metadata": {
        "id": "uHizf1B4W0Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperatures = [1, 0.5, 0.75, 1.1]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\nEvaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for temp in temperatures:\n",
        "        print(f\"Temperature: {temp}\")\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"./trained_models/\" \\\n",
        "            --loadWeights \"erfnet_pretrained.pth\" \\\n",
        "            --loadModel \"erfnet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 4 \\\n",
        "            --method \"MSP\" \\\n",
        "            --temp {temp}\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wjPLN4lX1jS",
        "outputId": "07ea8209-876c-4b61-87be-27870697cf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "Temperature: 1\n",
            "AUPRC score: 29.095121622764324\n",
            "FPR@TPR95: 62.548789859326625\n",
            "\n",
            "\n",
            "Temperature: 0.5\n",
            "AUPRC score: 27.060832506682093\n",
            "FPR@TPR95: 62.730810427606734\n",
            "\n",
            "\n",
            "Temperature: 0.75\n",
            "AUPRC score: 28.155279219746877\n",
            "FPR@TPR95: 62.48680866415934\n",
            "\n",
            "\n",
            "Temperature: 1.1\n",
            "AUPRC score: 29.40109502990521\n",
            "FPR@TPR95: 62.64680449239919\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "Temperature: 1\n",
            "AUPRC score: 2.7099746270831964\n",
            "FPR@TPR95: 65.22309081830707\n",
            "\n",
            "\n",
            "Temperature: 0.5\n",
            "AUPRC score: 2.4195502215405207\n",
            "FPR@TPR95: 63.22544524787239\n",
            "\n",
            "\n",
            "Temperature: 0.75\n",
            "AUPRC score: 2.5665765762205877\n",
            "FPR@TPR95: 64.1288367459823\n",
            "\n",
            "\n",
            "Temperature: 1.1\n",
            "AUPRC score: 2.763091863970154\n",
            "FPR@TPR95: 65.86731905563137\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "Temperature: 1\n",
            "AUPRC score: 7.472042122551045\n",
            "FPR@TPR95: 41.83689056458776\n",
            "\n",
            "\n",
            "Temperature: 0.5\n",
            "AUPRC score: 6.60119982404612\n",
            "FPR@TPR95: 43.47565874225287\n",
            "\n",
            "\n",
            "Temperature: 0.75\n",
            "AUPRC score: 6.99102031222911\n",
            "FPR@TPR95: 42.49932670115177\n",
            "\n",
            "\n",
            "Temperature: 1.1\n",
            "AUPRC score: 7.690238094059257\n",
            "FPR@TPR95: 41.62191102506234\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "Temperature: 1\n",
            "AUPRC score: 1.7490388138046746\n",
            "FPR@TPR95: 50.59403252456551\n",
            "\n",
            "\n",
            "Temperature: 0.5\n",
            "AUPRC score: 1.2802520336004974\n",
            "FPR@TPR95: 66.73710676943257\n",
            "\n",
            "\n",
            "Temperature: 0.75\n",
            "AUPRC score: 1.4929864225271197\n",
            "FPR@TPR95: 51.76136514478542\n",
            "\n",
            "\n",
            "Temperature: 1.1\n",
            "AUPRC score: 1.8614808647279362\n",
            "FPR@TPR95: 50.17685456318127\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "Temperature: 1\n",
            "AUPRC score: 12.42265113945068\n",
            "FPR@TPR95: 82.57546412366915\n",
            "\n",
            "\n",
            "Temperature: 0.5\n",
            "AUPRC score: 12.187675943363999\n",
            "FPR@TPR95: 82.02224728951396\n",
            "\n",
            "\n",
            "Temperature: 0.75\n",
            "AUPRC score: 12.318221374838616\n",
            "FPR@TPR95: 82.31442670002865\n",
            "\n",
            "\n",
            "Temperature: 1.1\n",
            "AUPRC score: 12.460467813627014\n",
            "FPR@TPR95: 82.73353126265913\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Void Classifiers"
      ],
      "metadata": {
        "id": "ORO2n8Gw9jXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENet Training"
      ],
      "metadata": {
        "id": "sDQjdzEttR8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd \"./train\" && python3 main.py \\\n",
        "--model enet \\\n",
        "--datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "--state \"../trained_models/enet_pretrained.pth\" \\\n",
        "--savedir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "--num-epochs=20 \\\n",
        "--decoder"
      ],
      "metadata": {
        "id": "dKuDsNFEvlvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENet Evaluation"
      ],
      "metadata": {
        "id": "h8rWTUIPvyBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 eval/eval_iou.py \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "    --loadWeights \"model_best.pth\" \\\n",
        "    --loadModel \"./train/enet.py\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4 \\\n",
        "    --void"
      ],
      "metadata": {
        "collapsed": true,
        "id": "70OoCVPZv1oK",
        "outputId": "052663dc-64df-4531-ac5c-fa5a7abb8f42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 351804\n",
            "Pruned percentage: 0.00%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 4.18 GFLOPs\n",
            "Estimated time: 0.006523 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:26<00:00,  5.76it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 42733.61it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0506 seconds\n",
            "Total inference time (model only): 25.29 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m90.05\u001b[0m Road\n",
            "\u001b[0m62.12\u001b[0m sidewalk\n",
            "\u001b[0m80.76\u001b[0m building\n",
            "\u001b[0m30.45\u001b[0m wall\n",
            "\u001b[0m25.89\u001b[0m fence\n",
            "\u001b[0m36.77\u001b[0m pole\n",
            "\u001b[0m21.91\u001b[0m traffic light\n",
            "\u001b[0m40.60\u001b[0m traffic sign\n",
            "\u001b[0m85.62\u001b[0m vegetation\n",
            "\u001b[0m47.47\u001b[0m terrain\n",
            "\u001b[0m87.94\u001b[0m sky\n",
            "\u001b[0m47.85\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m83.70\u001b[0m car\n",
            "\u001b[0m11.74\u001b[0m truck\n",
            "\u001b[0m29.36\u001b[0m bus\n",
            "\u001b[0m6.48\u001b[0m train\n",
            "\u001b[0m0.16\u001b[0m motorcycle\n",
            "\u001b[0m47.22\u001b[0m bicycle\n",
            "\u001b[0m60.63\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m44.84\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\n Evaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"/content/drive/MyDrive/training_results/enet/\"\\\n",
        "            --loadWeights 'model_best.pth' \\\n",
        "            --loadModel \"./train/enet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 2 \\\n",
        "            --method \"{method}\"\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "97mW0hm9v45s",
        "outputId": "523bc4b2-c226-4fdc-9acb-4a7b5b6c9ebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.1246 seconds\n",
            "Total inference time (model only): 1.25 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 14.184560580902508\n",
            "FPR@TPR95: 85.48476025081688\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0537 seconds\n",
            "Total inference time (model only): 1.61 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 1.4893736293517825\n",
            "FPR@TPR95: 51.65161833092648\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0513 seconds\n",
            "Total inference time (model only): 1.54 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 5.166231208501724\n",
            "FPR@TPR95: 42.9973200544032\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0405 seconds\n",
            "Total inference time (model only): 4.05 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 3.714776451212594\n",
            "FPR@TPR95: 75.08668291728215\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0430 seconds\n",
            "Total inference time (model only): 2.58 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 8.316994930194053\n",
            "FPR@TPR95: 79.48417613857553\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ErfNet Training"
      ],
      "metadata": {
        "id": "4gUyc7xJ_Wbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd \"./train\" && python3 main.py \\\n",
        "    --model erfnet \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --state \"../trained_models/erfnet_pretrained.pth\" \\\n",
        "    --savedir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\"\\\n",
        "    --num-epochs 40 \\\n",
        "    --decoder"
      ],
      "metadata": {
        "id": "si7MXzvz_V9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ErfNet Evaluation"
      ],
      "metadata": {
        "id": "VGrGlsl2w3b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./eval/eval_iou.py \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\" \\\n",
        "    --loadWeights \"model_best.pth\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4 \\\n",
        "    --void"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSD7bybw0Jjs",
        "outputId": "4424c5c0-f810-4a5d-dc1e-15ea71f17972",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 2066836\n",
            "Pruned percentage: 0.00%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 26.93 GFLOPs\n",
            "Estimated time: 0.042004 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:27<00:00,  5.72it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0425 seconds\n",
            "Total inference time (model only): 21.25 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m93.34\u001b[0m Road\n",
            "\u001b[0m70.43\u001b[0m sidewalk\n",
            "\u001b[0m87.15\u001b[0m building\n",
            "\u001b[0m42.11\u001b[0m wall\n",
            "\u001b[0m49.53\u001b[0m fence\n",
            "\u001b[0m55.74\u001b[0m pole\n",
            "\u001b[0m58.10\u001b[0m traffic light\n",
            "\u001b[0m66.28\u001b[0m traffic sign\n",
            "\u001b[0m89.97\u001b[0m vegetation\n",
            "\u001b[0m54.97\u001b[0m terrain\n",
            "\u001b[0m90.16\u001b[0m sky\n",
            "\u001b[0m72.74\u001b[0m person\n",
            "\u001b[0m54.05\u001b[0m rider\n",
            "\u001b[0m91.00\u001b[0m car\n",
            "\u001b[0m67.26\u001b[0m truck\n",
            "\u001b[0m77.52\u001b[0m bus\n",
            "\u001b[0m65.65\u001b[0m train\n",
            "\u001b[0m41.48\u001b[0m motorcycle\n",
            "\u001b[0m66.87\u001b[0m bicycle\n",
            "\u001b[0m67.32\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m68.08\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\n Evaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\"\\\n",
        "            --loadWeights 'model_best.pth' \\\n",
        "            --loadModel \"erfnet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 2 \\\n",
        "            --method \"{method}\"\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlh8YiEZ8I8y",
        "outputId": "783dc5d1-35f6-4344-fce2-641d7ccd2518",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "AUPRC score: 24.944169477218498\n",
            "FPR@TPR95: 82.87620155256329\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "AUPRC score: 2.013587082784733\n",
            "FPR@TPR95: 44.226788245737225\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "AUPRC score: 17.251392643411197\n",
            "FPR@TPR95: 34.84694044774548\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "AUPRC score: 4.398606158527537\n",
            "FPR@TPR95: 46.324246511318876\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "AUPRC score: 11.261848276422224\n",
            "FPR@TPR95: 84.46297224607517\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiseNet Training"
      ],
      "metadata": {
        "id": "i63NCiy5pmwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd \"./train\" && python3 main.py \\\n",
        "--model bisenet \\\n",
        "--datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "--state \"../trained_models/bisenet_pretrained.pth\" \\\n",
        "--savedir \"/content/drive/MyDrive/training_results/bisenet/\" \\\n",
        "--num-epochs=20 \\\n",
        "--decoder"
      ],
      "metadata": {
        "id": "Qj2yALSIpsnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiseNet Evaluation"
      ],
      "metadata": {
        "id": "5tijO4zwp21h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 eval/eval_iou.py \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/bisenet/\" \\\n",
        "    --loadWeights \"model_best.pth\" \\\n",
        "    --loadModel \"./train/bisenet.py\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4 \\\n",
        "    --void"
      ],
      "metadata": {
        "id": "QA9Y29P_p_z5",
        "outputId": "3ef4d243-60dd-4bdc-980a-94022742191d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 13426108\n",
            "Pruned percentage: 0.00%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 30.57 GFLOPs\n",
            "Estimated time: 0.047679 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [03:00<00:00,  2.77it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 42799.02it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0271 seconds\n",
            "Total inference time (model only): 13.57 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.86\u001b[0m Road\n",
            "\u001b[0m67.53\u001b[0m sidewalk\n",
            "\u001b[0m84.17\u001b[0m building\n",
            "\u001b[0m40.50\u001b[0m wall\n",
            "\u001b[0m40.93\u001b[0m fence\n",
            "\u001b[0m40.89\u001b[0m pole\n",
            "\u001b[0m46.80\u001b[0m traffic light\n",
            "\u001b[0m54.83\u001b[0m traffic sign\n",
            "\u001b[0m86.95\u001b[0m vegetation\n",
            "\u001b[0m52.14\u001b[0m terrain\n",
            "\u001b[0m86.02\u001b[0m sky\n",
            "\u001b[0m63.10\u001b[0m person\n",
            "\u001b[0m42.70\u001b[0m rider\n",
            "\u001b[0m88.43\u001b[0m car\n",
            "\u001b[0m53.91\u001b[0m truck\n",
            "\u001b[0m66.91\u001b[0m bus\n",
            "\u001b[0m47.11\u001b[0m train\n",
            "\u001b[0m31.11\u001b[0m motorcycle\n",
            "\u001b[0m60.62\u001b[0m bicycle\n",
            "\u001b[0m67.30\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.74\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\n Evaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"/content/drive/MyDrive/training_results/bisenet/\"\\\n",
        "            --loadWeights 'model_best.pth' \\\n",
        "            --loadModel \"./train/bisenet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 2 \\\n",
        "            --method \"{method}\"\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E8UAWz-qqM_N",
        "outputId": "3a0b3f43-6a30-457a-8aa4-a62eabcea20a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.1837 seconds\n",
            "Total inference time (model only): 1.84 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 22.737828903933217\n",
            "FPR@TPR95: 69.6159972425266\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0755 seconds\n",
            "Total inference time (model only): 2.26 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.6570856834099486\n",
            "FPR@TPR95: 22.25310599636985\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0778 seconds\n",
            "Total inference time (model only): 2.33 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 5.393242154867739\n",
            "FPR@TPR95: 49.773649316949765\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0411 seconds\n",
            "Total inference time (model only): 4.11 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 10.052456739686056\n",
            "FPR@TPR95: 31.61548230166732\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "=======================================\n",
            "Avg inference time per image: 0.0510 seconds\n",
            "Total inference time (model only): 3.06 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 13.80198171205644\n",
            "FPR@TPR95: 82.74734066261423\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extension\n"
      ],
      "metadata": {
        "id": "wZ79PP2-7yj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning"
      ],
      "metadata": {
        "id": "qAtBEh9t727E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENet Pruning"
      ],
      "metadata": {
        "id": "NRSklGzuy1Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pruning in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    print(f\"\\nRunning evaluation with pruning={pruning}\")\n",
        "    !python3 ./eval/eval_iou.py \\\n",
        "        --loadDir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "        --loadWeights \"model_best.pth\" \\\n",
        "        --loadModel \"./train/enet.py\" \\\n",
        "        --subset val \\\n",
        "        --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "        --batch-size 1 \\\n",
        "        --num-workers 4 \\\n",
        "        --void \\\n",
        "        --pruning {pruning}"
      ],
      "metadata": {
        "id": "1ncJF82ry315",
        "outputId": "930c6507-f737-4fb9-eca3-0606e3125b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running evaluation with pruning=0.1\n",
            "Applying unstructured L1 pruning with 10.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 318284\n",
            "Pruned percentage: 9.53%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 3.78 GFLOPs\n",
            "Estimated time: 0.005902 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:24<00:00,  5.93it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 39662.45it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0498 seconds\n",
            "Total inference time (model only): 24.88 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m89.97\u001b[0m Road\n",
            "\u001b[0m61.15\u001b[0m sidewalk\n",
            "\u001b[0m80.73\u001b[0m building\n",
            "\u001b[0m32.61\u001b[0m wall\n",
            "\u001b[0m25.74\u001b[0m fence\n",
            "\u001b[0m36.43\u001b[0m pole\n",
            "\u001b[0m20.84\u001b[0m traffic light\n",
            "\u001b[0m40.57\u001b[0m traffic sign\n",
            "\u001b[0m85.46\u001b[0m vegetation\n",
            "\u001b[0m47.33\u001b[0m terrain\n",
            "\u001b[0m88.02\u001b[0m sky\n",
            "\u001b[0m49.02\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m83.75\u001b[0m car\n",
            "\u001b[0m11.89\u001b[0m truck\n",
            "\u001b[0m29.63\u001b[0m bus\n",
            "\u001b[0m7.24\u001b[0m train\n",
            "\u001b[0m0.11\u001b[0m motorcycle\n",
            "\u001b[0m46.74\u001b[0m bicycle\n",
            "\u001b[0m60.74\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m44.90\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.2\n",
            "Applying unstructured L1 pruning with 20.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 284788\n",
            "Pruned percentage: 19.05%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 3.39 GFLOPs\n",
            "Estimated time: 0.005281 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:20<00:00,  6.22it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 30693.77it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0515 seconds\n",
            "Total inference time (model only): 25.73 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m89.23\u001b[0m Road\n",
            "\u001b[0m57.48\u001b[0m sidewalk\n",
            "\u001b[0m77.53\u001b[0m building\n",
            "\u001b[0m32.03\u001b[0m wall\n",
            "\u001b[0m20.95\u001b[0m fence\n",
            "\u001b[0m33.29\u001b[0m pole\n",
            "\u001b[0m14.37\u001b[0m traffic light\n",
            "\u001b[0m37.61\u001b[0m traffic sign\n",
            "\u001b[0m84.07\u001b[0m vegetation\n",
            "\u001b[0m45.34\u001b[0m terrain\n",
            "\u001b[0m73.69\u001b[0m sky\n",
            "\u001b[0m46.46\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m81.33\u001b[0m car\n",
            "\u001b[0m10.09\u001b[0m truck\n",
            "\u001b[0m26.83\u001b[0m bus\n",
            "\u001b[0m4.46\u001b[0m train\n",
            "\u001b[0m0.06\u001b[0m motorcycle\n",
            "\u001b[0m43.82\u001b[0m bicycle\n",
            "\u001b[0m60.22\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m41.94\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.3\n",
            "Applying unstructured L1 pruning with 30.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 251269\n",
            "Pruned percentage: 28.58%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 2.99 GFLOPs\n",
            "Estimated time: 0.004659 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:22<00:00,  6.06it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 35261.07it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0529 seconds\n",
            "Total inference time (model only): 26.44 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m77.26\u001b[0m Road\n",
            "\u001b[0m32.72\u001b[0m sidewalk\n",
            "\u001b[0m45.71\u001b[0m building\n",
            "\u001b[0m4.53\u001b[0m wall\n",
            "\u001b[0m4.46\u001b[0m fence\n",
            "\u001b[0m13.29\u001b[0m pole\n",
            "\u001b[0m1.11\u001b[0m traffic light\n",
            "\u001b[0m11.31\u001b[0m traffic sign\n",
            "\u001b[0m63.46\u001b[0m vegetation\n",
            "\u001b[0m28.36\u001b[0m terrain\n",
            "\u001b[0m1.41\u001b[0m sky\n",
            "\u001b[0m20.96\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m47.04\u001b[0m car\n",
            "\u001b[0m5.34\u001b[0m truck\n",
            "\u001b[0m4.88\u001b[0m bus\n",
            "\u001b[0m0.54\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m16.58\u001b[0m bicycle\n",
            "\u001b[0m46.08\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m21.25\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.4\n",
            "Applying unstructured L1 pruning with 40.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 217773\n",
            "Pruned percentage: 38.10%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 2.59 GFLOPs\n",
            "Estimated time: 0.004038 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:21<00:00,  6.11it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 43577.18it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0525 seconds\n",
            "Total inference time (model only): 26.23 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m1.66\u001b[0m Road\n",
            "\u001b[0m0.08\u001b[0m sidewalk\n",
            "\u001b[0m19.60\u001b[0m building\n",
            "\u001b[0m1.58\u001b[0m wall\n",
            "\u001b[0m1.74\u001b[0m fence\n",
            "\u001b[0m6.85\u001b[0m pole\n",
            "\u001b[0m0.00\u001b[0m traffic light\n",
            "\u001b[0m0.14\u001b[0m traffic sign\n",
            "\u001b[0m13.68\u001b[0m vegetation\n",
            "\u001b[0m10.16\u001b[0m terrain\n",
            "\u001b[0m2.70\u001b[0m sky\n",
            "\u001b[0m1.25\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m1.05\u001b[0m car\n",
            "\u001b[0m0.07\u001b[0m truck\n",
            "\u001b[0m0.02\u001b[0m bus\n",
            "\u001b[0m0.00\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m0.11\u001b[0m bicycle\n",
            "\u001b[0m22.42\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m4.16\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.5\n",
            "Applying unstructured L1 pruning with 50.0% sparsity to Conv2d layers...\n",
            "Total parameters: 351804 | Non-zero (effective) parameters after pruning: 184252\n",
            "Pruned percentage: 47.63%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::prelu encountered 93 time(s)\n",
            "Unsupported operator aten::max_pool2d_with_indices encountered 2 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 27 time(s)\n",
            "Unsupported operator aten::sub encountered 2 time(s)\n",
            "Unsupported operator aten::add encountered 27 time(s)\n",
            "Unsupported operator aten::max_unpool2d encountered 2 time(s)\n",
            "Total FLOPs: 2.19 GFLOPs\n",
            "Estimated time: 0.003416 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:22<00:00,  6.07it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 45939.80it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0528 seconds\n",
            "Total inference time (model only): 26.38 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m0.20\u001b[0m Road\n",
            "\u001b[0m0.00\u001b[0m sidewalk\n",
            "\u001b[0m12.43\u001b[0m building\n",
            "\u001b[0m1.20\u001b[0m wall\n",
            "\u001b[0m2.02\u001b[0m fence\n",
            "\u001b[0m3.24\u001b[0m pole\n",
            "\u001b[0m0.00\u001b[0m traffic light\n",
            "\u001b[0m0.00\u001b[0m traffic sign\n",
            "\u001b[0m44.84\u001b[0m vegetation\n",
            "\u001b[0m5.09\u001b[0m terrain\n",
            "\u001b[0m0.00\u001b[0m sky\n",
            "\u001b[0m0.09\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m6.93\u001b[0m car\n",
            "\u001b[0m0.06\u001b[0m truck\n",
            "\u001b[0m0.09\u001b[0m bus\n",
            "\u001b[0m0.00\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m0.01\u001b[0m bicycle\n",
            "\u001b[0m21.94\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m4.91\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "pruning_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\nEvaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        for pruning in pruning_values:\n",
        "            print(f\"\\n--> Pruning: {pruning}\")\n",
        "            !python3 eval/evalAnomaly.py \\\n",
        "                --input \"{input_path}\" \\\n",
        "                --loadDir \"/content/drive/MyDrive/training_results/enet/\" \\\n",
        "                --loadWeights 'model_best.pth' \\\n",
        "                --loadModel \"./train/enet.py\" \\\n",
        "                --subset val \\\n",
        "                --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "                --batch-size 1 \\\n",
        "                --num-workers 2 \\\n",
        "                --method \"{method}\" \\\n",
        "                --pruning \"{pruning}\"\n",
        "            print(\"\\n\")"
      ],
      "metadata": {
        "id": "CTFK7VFqy4Ha",
        "outputId": "617e6463-1e7b-474d-e0b4-7de6d0fd8d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.1200 seconds\n",
            "Total inference time (model only): 1.20 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 14.204289129859141\n",
            "FPR@TPR95: 84.43342712794347\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.1176 seconds\n",
            "Total inference time (model only): 1.18 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 13.302121658240859\n",
            "FPR@TPR95: 90.08068068397773\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.1061 seconds\n",
            "Total inference time (model only): 1.06 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 12.42731899429767\n",
            "FPR@TPR95: 93.77949444620086\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.1044 seconds\n",
            "Total inference time (model only): 1.04 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 15.43261095628616\n",
            "FPR@TPR95: 85.85700060284938\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.1062 seconds\n",
            "Total inference time (model only): 1.06 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 13.479004464687225\n",
            "FPR@TPR95: 89.96574730010927\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0534 seconds\n",
            "Total inference time (model only): 1.60 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 1.2831549117964733\n",
            "FPR@TPR95: 43.353097384703034\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0556 seconds\n",
            "Total inference time (model only): 1.67 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 0.5454819090519771\n",
            "FPR@TPR95: 88.02707127147949\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0563 seconds\n",
            "Total inference time (model only): 1.69 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 0.398156450040856\n",
            "FPR@TPR95: 98.90296473191219\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0591 seconds\n",
            "Total inference time (model only): 1.77 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 0.5504337612469938\n",
            "FPR@TPR95: 94.28448227321506\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0606 seconds\n",
            "Total inference time (model only): 1.82 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 0.5692803424075917\n",
            "FPR@TPR95: 91.66452755070948\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0520 seconds\n",
            "Total inference time (model only): 1.56 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 5.212369448995486\n",
            "FPR@TPR95: 42.46638677828419\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0505 seconds\n",
            "Total inference time (model only): 1.51 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 5.254631844811212\n",
            "FPR@TPR95: 47.73968764884228\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0576 seconds\n",
            "Total inference time (model only): 1.73 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.2462202904950255\n",
            "FPR@TPR95: 59.36089957282192\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0529 seconds\n",
            "Total inference time (model only): 1.59 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 2.246860023553497\n",
            "FPR@TPR95: 89.7734122153357\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0525 seconds\n",
            "Total inference time (model only): 1.58 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 2.7439526074818104\n",
            "FPR@TPR95: 84.52453862234199\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0420 seconds\n",
            "Total inference time (model only): 4.20 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 3.829281825480883\n",
            "FPR@TPR95: 73.65974678599088\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0417 seconds\n",
            "Total inference time (model only): 4.17 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 3.5668988535899606\n",
            "FPR@TPR95: 62.02347423347704\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0411 seconds\n",
            "Total inference time (model only): 4.11 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 2.035346020753192\n",
            "FPR@TPR95: 70.50456061906847\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0404 seconds\n",
            "Total inference time (model only): 4.04 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 1.1499937302644117\n",
            "FPR@TPR95: 94.11887384906447\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0402 seconds\n",
            "Total inference time (model only): 4.02 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 0.3209089114263293\n",
            "FPR@TPR95: 94.68335259702177\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0406 seconds\n",
            "Total inference time (model only): 2.44 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 8.42156755560816\n",
            "FPR@TPR95: 80.52810781670856\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0398 seconds\n",
            "Total inference time (model only): 2.39 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 7.620328182457523\n",
            "FPR@TPR95: 89.16230801823932\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0389 seconds\n",
            "Total inference time (model only): 2.34 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 7.347193676285696\n",
            "FPR@TPR95: 94.72332961340169\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0391 seconds\n",
            "Total inference time (model only): 2.35 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 8.301330814945981\n",
            "FPR@TPR95: 94.89825855292779\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0407 seconds\n",
            "Total inference time (model only): 2.44 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 7.297218676970896\n",
            "FPR@TPR95: 97.52190420066417\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ErfNet Pruning"
      ],
      "metadata": {
        "id": "8sPBynhGxpJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pruning in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    print(f\"\\nRunning evaluation with pruning={pruning}\")\n",
        "    !python3 ./eval/eval_iou.py \\\n",
        "        --loadDir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\" \\\n",
        "        --loadWeights \"model_best.pth\" \\\n",
        "        --loadModel \"erfnet.py\" \\\n",
        "        --subset val \\\n",
        "        --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "        --batch-size 1 \\\n",
        "        --num-workers 4 \\\n",
        "        --void \\\n",
        "        --pruning {pruning}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhwm5_CG7_Tb",
        "outputId": "8408fdbd-2863-43d5-fc12-ab26bba09abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running evaluation with pruning=0.1\n",
            "Applying unstructured L1 pruning with 10.0% sparsity to Conv2d layers...\n",
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 1869860\n",
            "Pruned percentage: 9.53%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 24.37 GFLOPs\n",
            "Estimated time: 0.038001 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:27<00:00,  5.74it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0420 seconds\n",
            "Total inference time (model only): 20.99 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m93.39\u001b[0m Road\n",
            "\u001b[0m70.51\u001b[0m sidewalk\n",
            "\u001b[0m87.12\u001b[0m building\n",
            "\u001b[0m42.17\u001b[0m wall\n",
            "\u001b[0m49.51\u001b[0m fence\n",
            "\u001b[0m55.78\u001b[0m pole\n",
            "\u001b[0m57.85\u001b[0m traffic light\n",
            "\u001b[0m66.25\u001b[0m traffic sign\n",
            "\u001b[0m89.96\u001b[0m vegetation\n",
            "\u001b[0m55.05\u001b[0m terrain\n",
            "\u001b[0m90.17\u001b[0m sky\n",
            "\u001b[0m72.78\u001b[0m person\n",
            "\u001b[0m54.03\u001b[0m rider\n",
            "\u001b[0m91.04\u001b[0m car\n",
            "\u001b[0m67.37\u001b[0m truck\n",
            "\u001b[0m77.42\u001b[0m bus\n",
            "\u001b[0m65.46\u001b[0m train\n",
            "\u001b[0m41.51\u001b[0m motorcycle\n",
            "\u001b[0m66.98\u001b[0m bicycle\n",
            "\u001b[0m67.33\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m68.08\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.2\n",
            "Applying unstructured L1 pruning with 20.0% sparsity to Conv2d layers...\n",
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 1672883\n",
            "Pruned percentage: 19.06%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 21.80 GFLOPs\n",
            "Estimated time: 0.033998 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:29<00:00,  5.56it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0433 seconds\n",
            "Total inference time (model only): 21.66 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m93.32\u001b[0m Road\n",
            "\u001b[0m70.38\u001b[0m sidewalk\n",
            "\u001b[0m87.05\u001b[0m building\n",
            "\u001b[0m42.56\u001b[0m wall\n",
            "\u001b[0m49.45\u001b[0m fence\n",
            "\u001b[0m55.10\u001b[0m pole\n",
            "\u001b[0m57.39\u001b[0m traffic light\n",
            "\u001b[0m66.57\u001b[0m traffic sign\n",
            "\u001b[0m89.91\u001b[0m vegetation\n",
            "\u001b[0m55.33\u001b[0m terrain\n",
            "\u001b[0m90.56\u001b[0m sky\n",
            "\u001b[0m73.42\u001b[0m person\n",
            "\u001b[0m53.88\u001b[0m rider\n",
            "\u001b[0m91.12\u001b[0m car\n",
            "\u001b[0m66.49\u001b[0m truck\n",
            "\u001b[0m76.94\u001b[0m bus\n",
            "\u001b[0m64.96\u001b[0m train\n",
            "\u001b[0m41.87\u001b[0m motorcycle\n",
            "\u001b[0m66.54\u001b[0m bicycle\n",
            "\u001b[0m67.28\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m68.01\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.3\n",
            "Applying unstructured L1 pruning with 30.0% sparsity to Conv2d layers...\n",
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 1475910\n",
            "Pruned percentage: 28.59%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 19.23 GFLOPs\n",
            "Estimated time: 0.029995 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:27<00:00,  5.72it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0428 seconds\n",
            "Total inference time (model only): 21.41 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m93.19\u001b[0m Road\n",
            "\u001b[0m69.39\u001b[0m sidewalk\n",
            "\u001b[0m85.80\u001b[0m building\n",
            "\u001b[0m36.02\u001b[0m wall\n",
            "\u001b[0m46.40\u001b[0m fence\n",
            "\u001b[0m55.15\u001b[0m pole\n",
            "\u001b[0m57.29\u001b[0m traffic light\n",
            "\u001b[0m65.68\u001b[0m traffic sign\n",
            "\u001b[0m89.16\u001b[0m vegetation\n",
            "\u001b[0m53.68\u001b[0m terrain\n",
            "\u001b[0m87.81\u001b[0m sky\n",
            "\u001b[0m73.39\u001b[0m person\n",
            "\u001b[0m53.13\u001b[0m rider\n",
            "\u001b[0m89.92\u001b[0m car\n",
            "\u001b[0m55.83\u001b[0m truck\n",
            "\u001b[0m75.88\u001b[0m bus\n",
            "\u001b[0m60.22\u001b[0m train\n",
            "\u001b[0m39.90\u001b[0m motorcycle\n",
            "\u001b[0m65.87\u001b[0m bicycle\n",
            "\u001b[0m66.81\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m66.03\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.4\n",
            "Applying unstructured L1 pruning with 40.0% sparsity to Conv2d layers...\n",
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 1278933\n",
            "Pruned percentage: 38.12%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 16.67 GFLOPs\n",
            "Estimated time: 0.025992 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:28<00:00,  5.62it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0440 seconds\n",
            "Total inference time (model only): 22.01 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m8.63\u001b[0m Road\n",
            "\u001b[0m6.97\u001b[0m sidewalk\n",
            "\u001b[0m36.64\u001b[0m building\n",
            "\u001b[0m4.32\u001b[0m wall\n",
            "\u001b[0m15.63\u001b[0m fence\n",
            "\u001b[0m22.20\u001b[0m pole\n",
            "\u001b[0m32.66\u001b[0m traffic light\n",
            "\u001b[0m31.76\u001b[0m traffic sign\n",
            "\u001b[0m72.70\u001b[0m vegetation\n",
            "\u001b[0m6.82\u001b[0m terrain\n",
            "\u001b[0m0.02\u001b[0m sky\n",
            "\u001b[0m34.39\u001b[0m person\n",
            "\u001b[0m4.05\u001b[0m rider\n",
            "\u001b[0m23.68\u001b[0m car\n",
            "\u001b[0m11.14\u001b[0m truck\n",
            "\u001b[0m14.52\u001b[0m bus\n",
            "\u001b[0m13.87\u001b[0m train\n",
            "\u001b[0m6.89\u001b[0m motorcycle\n",
            "\u001b[0m32.11\u001b[0m bicycle\n",
            "\u001b[0m38.34\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m20.87\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.5\n",
            "Applying unstructured L1 pruning with 50.0% sparsity to Conv2d layers...\n",
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 1081956\n",
            "Pruned percentage: 47.65%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::add_ encountered 39 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 14.10 GFLOPs\n",
            "Estimated time: 0.021988 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:28<00:00,  5.68it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0425 seconds\n",
            "Total inference time (model only): 21.23 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m0.00\u001b[0m Road\n",
            "\u001b[0m0.00\u001b[0m sidewalk\n",
            "\u001b[0m19.85\u001b[0m building\n",
            "\u001b[0m0.00\u001b[0m wall\n",
            "\u001b[0m0.09\u001b[0m fence\n",
            "\u001b[0m0.05\u001b[0m pole\n",
            "\u001b[0m3.05\u001b[0m traffic light\n",
            "\u001b[0m1.87\u001b[0m traffic sign\n",
            "\u001b[0m15.06\u001b[0m vegetation\n",
            "\u001b[0m0.00\u001b[0m terrain\n",
            "\u001b[0m0.00\u001b[0m sky\n",
            "\u001b[0m0.65\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m0.72\u001b[0m car\n",
            "\u001b[0m0.00\u001b[0m truck\n",
            "\u001b[0m0.00\u001b[0m bus\n",
            "\u001b[0m0.00\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m0.00\u001b[0m bicycle\n",
            "\u001b[0m14.57\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m2.80\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "pruning_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\nEvaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        for pruning in pruning_values:\n",
        "            print(f\"\\n--> Pruning: {pruning}\")\n",
        "            !python3 eval/evalAnomaly.py \\\n",
        "                --input \"{input_path}\" \\\n",
        "                --loadDir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\" \\\n",
        "                --loadWeights 'model_best.pth' \\\n",
        "                --loadModel \"erfnet.py\" \\\n",
        "                --subset val \\\n",
        "                --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "                --batch-size 1 \\\n",
        "                --num-workers 2 \\\n",
        "                --method \"{method}\" \\\n",
        "                --pruning \"{pruning}\"\n",
        "            print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrgowb-zzGqw",
        "outputId": "817070a2-6d98-4e5a-ceb5-6afdf9f52caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "AUPRC score: 25.03619926144056\n",
            "FPR@TPR95: 82.77574212056848\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "AUPRC score: 24.418278964564408\n",
            "FPR@TPR95: 82.39554573409262\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "AUPRC score: 23.410579687603036\n",
            "FPR@TPR95: 80.87371639358294\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "AUPRC score: 22.265943542028165\n",
            "FPR@TPR95: 81.52251592540625\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "AUPRC score: 16.353138330889205\n",
            "FPR@TPR95: 89.55599992855807\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "AUPRC score: 2.0353835614053626\n",
            "FPR@TPR95: 44.309095907470955\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "AUPRC score: 1.9502492370276114\n",
            "FPR@TPR95: 44.096702394043376\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "AUPRC score: 2.3395969734213846\n",
            "FPR@TPR95: 42.11906209011778\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "AUPRC score: 1.1832899279090385\n",
            "FPR@TPR95: 65.98366764265558\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "AUPRC score: 0.7424842647437129\n",
            "FPR@TPR95: 90.24819610421441\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "AUPRC score: 17.310909037015218\n",
            "FPR@TPR95: 34.37981540012176\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "AUPRC score: 16.271208806549513\n",
            "FPR@TPR95: 34.17609223066825\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "AUPRC score: 19.175696712231673\n",
            "FPR@TPR95: 31.555435403408637\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "AUPRC score: 21.823594156174085\n",
            "FPR@TPR95: 35.7997402575063\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "AUPRC score: 3.1180444624091317\n",
            "FPR@TPR95: 86.35456965708366\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "AUPRC score: 4.391875262779421\n",
            "FPR@TPR95: 46.924447698409146\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "AUPRC score: 4.392826053339577\n",
            "FPR@TPR95: 45.52412865430476\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "AUPRC score: 3.8346331280608603\n",
            "FPR@TPR95: 41.929881535558735\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "AUPRC score: 1.0721763525635297\n",
            "FPR@TPR95: 53.190020736584486\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "AUPRC score: 0.5442047116533457\n",
            "FPR@TPR95: 73.95688979348094\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "AUPRC score: 11.291476297294786\n",
            "FPR@TPR95: 84.17674861236017\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "AUPRC score: 11.542794638834764\n",
            "FPR@TPR95: 84.05865958901714\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "AUPRC score: 12.984119034972007\n",
            "FPR@TPR95: 84.68793830969165\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "AUPRC score: 15.318160457537969\n",
            "FPR@TPR95: 75.92801057754478\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "AUPRC score: 10.045742877178549\n",
            "FPR@TPR95: 88.8361710506569\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiseNet Pruning"
      ],
      "metadata": {
        "id": "tu2TYM7zxy9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pruning in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
        "    print(f\"\\nRunning evaluation with pruning={pruning}\")\n",
        "    !python3 ./eval/eval_iou.py \\\n",
        "        --loadDir \"/content/drive/MyDrive/training_results/bisenet/\" \\\n",
        "        --loadWeights \"model_best.pth\" \\\n",
        "        --loadModel \"./train/bisenet.py\" \\\n",
        "        --subset val \\\n",
        "        --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "        --batch-size 1 \\\n",
        "        --num-workers 4 \\\n",
        "        --void \\\n",
        "        --pruning {pruning}"
      ],
      "metadata": {
        "id": "bEyG46pZx3PU",
        "outputId": "076ea3f6-1939-4884-ce85-c9d69724ddaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running evaluation with pruning=0.1\n",
            "Applying unstructured L1 pruning with 10.0% sparsity to Conv2d layers...\n",
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 12084886\n",
            "Pruned percentage: 9.99%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 27.52 GFLOPs\n",
            "Estimated time: 0.042916 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:21<00:00,  6.16it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 45565.50it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0305 seconds\n",
            "Total inference time (model only): 15.25 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.85\u001b[0m Road\n",
            "\u001b[0m67.63\u001b[0m sidewalk\n",
            "\u001b[0m84.03\u001b[0m building\n",
            "\u001b[0m40.65\u001b[0m wall\n",
            "\u001b[0m40.95\u001b[0m fence\n",
            "\u001b[0m40.79\u001b[0m pole\n",
            "\u001b[0m46.65\u001b[0m traffic light\n",
            "\u001b[0m54.86\u001b[0m traffic sign\n",
            "\u001b[0m86.91\u001b[0m vegetation\n",
            "\u001b[0m52.08\u001b[0m terrain\n",
            "\u001b[0m86.00\u001b[0m sky\n",
            "\u001b[0m63.06\u001b[0m person\n",
            "\u001b[0m42.82\u001b[0m rider\n",
            "\u001b[0m88.37\u001b[0m car\n",
            "\u001b[0m53.46\u001b[0m truck\n",
            "\u001b[0m66.80\u001b[0m bus\n",
            "\u001b[0m48.88\u001b[0m train\n",
            "\u001b[0m31.77\u001b[0m motorcycle\n",
            "\u001b[0m60.34\u001b[0m bicycle\n",
            "\u001b[0m67.29\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.81\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.2\n",
            "Applying unstructured L1 pruning with 20.0% sparsity to Conv2d layers...\n",
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 10743662\n",
            "Pruned percentage: 19.98%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 24.46 GFLOPs\n",
            "Estimated time: 0.038153 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:22<00:00,  6.06it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 27666.91it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0307 seconds\n",
            "Total inference time (model only): 15.34 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.73\u001b[0m Road\n",
            "\u001b[0m66.97\u001b[0m sidewalk\n",
            "\u001b[0m83.66\u001b[0m building\n",
            "\u001b[0m40.07\u001b[0m wall\n",
            "\u001b[0m41.64\u001b[0m fence\n",
            "\u001b[0m40.38\u001b[0m pole\n",
            "\u001b[0m46.50\u001b[0m traffic light\n",
            "\u001b[0m54.86\u001b[0m traffic sign\n",
            "\u001b[0m86.81\u001b[0m vegetation\n",
            "\u001b[0m52.91\u001b[0m terrain\n",
            "\u001b[0m85.86\u001b[0m sky\n",
            "\u001b[0m62.28\u001b[0m person\n",
            "\u001b[0m43.00\u001b[0m rider\n",
            "\u001b[0m88.29\u001b[0m car\n",
            "\u001b[0m51.55\u001b[0m truck\n",
            "\u001b[0m67.88\u001b[0m bus\n",
            "\u001b[0m51.67\u001b[0m train\n",
            "\u001b[0m32.15\u001b[0m motorcycle\n",
            "\u001b[0m60.09\u001b[0m bicycle\n",
            "\u001b[0m67.30\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.83\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.3\n",
            "Applying unstructured L1 pruning with 30.0% sparsity to Conv2d layers...\n",
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 9402442\n",
            "Pruned percentage: 29.97%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 21.41 GFLOPs\n",
            "Estimated time: 0.033390 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:22<00:00,  6.03it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 46141.96it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0313 seconds\n",
            "Total inference time (model only): 15.63 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.40\u001b[0m Road\n",
            "\u001b[0m65.59\u001b[0m sidewalk\n",
            "\u001b[0m83.55\u001b[0m building\n",
            "\u001b[0m40.56\u001b[0m wall\n",
            "\u001b[0m41.27\u001b[0m fence\n",
            "\u001b[0m39.77\u001b[0m pole\n",
            "\u001b[0m46.00\u001b[0m traffic light\n",
            "\u001b[0m54.16\u001b[0m traffic sign\n",
            "\u001b[0m86.73\u001b[0m vegetation\n",
            "\u001b[0m52.08\u001b[0m terrain\n",
            "\u001b[0m86.08\u001b[0m sky\n",
            "\u001b[0m61.46\u001b[0m person\n",
            "\u001b[0m41.06\u001b[0m rider\n",
            "\u001b[0m88.01\u001b[0m car\n",
            "\u001b[0m50.60\u001b[0m truck\n",
            "\u001b[0m66.07\u001b[0m bus\n",
            "\u001b[0m54.48\u001b[0m train\n",
            "\u001b[0m27.94\u001b[0m motorcycle\n",
            "\u001b[0m60.24\u001b[0m bicycle\n",
            "\u001b[0m65.99\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.20\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.4\n",
            "Applying unstructured L1 pruning with 40.0% sparsity to Conv2d layers...\n",
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 8061218\n",
            "Pruned percentage: 39.96%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 18.36 GFLOPs\n",
            "Estimated time: 0.028627 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:21<00:00,  6.10it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 45789.34it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0308 seconds\n",
            "Total inference time (model only): 15.39 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m91.74\u001b[0m Road\n",
            "\u001b[0m62.26\u001b[0m sidewalk\n",
            "\u001b[0m80.12\u001b[0m building\n",
            "\u001b[0m31.48\u001b[0m wall\n",
            "\u001b[0m30.77\u001b[0m fence\n",
            "\u001b[0m38.08\u001b[0m pole\n",
            "\u001b[0m43.56\u001b[0m traffic light\n",
            "\u001b[0m52.86\u001b[0m traffic sign\n",
            "\u001b[0m86.20\u001b[0m vegetation\n",
            "\u001b[0m48.64\u001b[0m terrain\n",
            "\u001b[0m64.08\u001b[0m sky\n",
            "\u001b[0m57.19\u001b[0m person\n",
            "\u001b[0m35.35\u001b[0m rider\n",
            "\u001b[0m87.73\u001b[0m car\n",
            "\u001b[0m55.63\u001b[0m truck\n",
            "\u001b[0m62.69\u001b[0m bus\n",
            "\u001b[0m46.66\u001b[0m train\n",
            "\u001b[0m26.35\u001b[0m motorcycle\n",
            "\u001b[0m58.76\u001b[0m bicycle\n",
            "\u001b[0m64.59\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m56.24\u001b[0m %\n",
            "\n",
            "Running evaluation with pruning=0.5\n",
            "Applying unstructured L1 pruning with 50.0% sparsity to Conv2d layers...\n",
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 6719996\n",
            "Pruned percentage: 49.95%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 15.30 GFLOPs\n",
            "Estimated time: 0.023864 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 500/500 [01:22<00:00,  6.08it/s]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 42930.44it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.0305 seconds\n",
            "Total inference time (model only): 15.23 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m61.80\u001b[0m Road\n",
            "\u001b[0m23.40\u001b[0m sidewalk\n",
            "\u001b[0m71.86\u001b[0m building\n",
            "\u001b[0m5.73\u001b[0m wall\n",
            "\u001b[0m4.26\u001b[0m fence\n",
            "\u001b[0m30.18\u001b[0m pole\n",
            "\u001b[0m36.47\u001b[0m traffic light\n",
            "\u001b[0m40.65\u001b[0m traffic sign\n",
            "\u001b[0m73.32\u001b[0m vegetation\n",
            "\u001b[0m26.16\u001b[0m terrain\n",
            "\u001b[0m31.66\u001b[0m sky\n",
            "\u001b[0m54.19\u001b[0m person\n",
            "\u001b[0m23.63\u001b[0m rider\n",
            "\u001b[0m81.27\u001b[0m car\n",
            "\u001b[0m24.07\u001b[0m truck\n",
            "\u001b[0m19.62\u001b[0m bus\n",
            "\u001b[0m14.65\u001b[0m train\n",
            "\u001b[0m11.88\u001b[0m motorcycle\n",
            "\u001b[0m43.83\u001b[0m bicycle\n",
            "\u001b[0m49.66\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m36.41\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "pruning_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\nEvaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        for pruning in pruning_values:\n",
        "            print(f\"\\n--> Pruning: {pruning}\")\n",
        "            !python3 eval/evalAnomaly.py \\\n",
        "                --input \"{input_path}\" \\\n",
        "                --loadDir \"/content/drive/MyDrive/training_results/bisenet/\" \\\n",
        "                --loadWeights 'model_best.pth' \\\n",
        "                --loadModel \"./train/bisenet.py\" \\\n",
        "                --subset val \\\n",
        "                --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "                --batch-size 1 \\\n",
        "                --num-workers 2 \\\n",
        "                --method \"{method}\" \\\n",
        "                --pruning \"{pruning}\"\n",
        "            print(\"\\n\")"
      ],
      "metadata": {
        "id": "xBK7J7Hvx8fZ",
        "outputId": "3f9e1a61-592e-4ef0-a9b4-8c04fec1107f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.1787 seconds\n",
            "Total inference time (model only): 1.79 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 22.649562255228446\n",
            "FPR@TPR95: 69.61298183588626\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.1672 seconds\n",
            "Total inference time (model only): 1.67 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 21.34106707569094\n",
            "FPR@TPR95: 68.85859668077954\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.1642 seconds\n",
            "Total inference time (model only): 1.64 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 20.940581248795198\n",
            "FPR@TPR95: 71.8812402970593\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.1654 seconds\n",
            "Total inference time (model only): 1.65 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 19.321847079662472\n",
            "FPR@TPR95: 83.57053372465579\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.1668 seconds\n",
            "Total inference time (model only): 1.67 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 12.720236830802726\n",
            "FPR@TPR95: 86.2267358478268\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0690 seconds\n",
            "Total inference time (model only): 2.07 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.70076124747973\n",
            "FPR@TPR95: 21.530968050716094\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0707 seconds\n",
            "Total inference time (model only): 2.12 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.228060693896352\n",
            "FPR@TPR95: 23.702039640164816\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0747 seconds\n",
            "Total inference time (model only): 2.24 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.3364944194598\n",
            "FPR@TPR95: 21.08705318697916\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0722 seconds\n",
            "Total inference time (model only): 2.17 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 3.073937164804464\n",
            "FPR@TPR95: 27.485086911591306\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0687 seconds\n",
            "Total inference time (model only): 2.06 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 1.0824249514884434\n",
            "FPR@TPR95: 79.9742519442494\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0645 seconds\n",
            "Total inference time (model only): 1.94 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 5.478952167657349\n",
            "FPR@TPR95: 49.15094376902769\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0668 seconds\n",
            "Total inference time (model only): 2.00 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.662904896564032\n",
            "FPR@TPR95: 51.53087446562061\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0642 seconds\n",
            "Total inference time (model only): 1.93 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.69287718459773\n",
            "FPR@TPR95: 50.967095642374524\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0642 seconds\n",
            "Total inference time (model only): 1.93 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 5.328469814146391\n",
            "FPR@TPR95: 54.64612235419587\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0665 seconds\n",
            "Total inference time (model only): 2.00 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 2.611123781050613\n",
            "FPR@TPR95: 88.08379589456231\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0391 seconds\n",
            "Total inference time (model only): 3.91 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 9.962891064540951\n",
            "FPR@TPR95: 31.861783680612433\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0398 seconds\n",
            "Total inference time (model only): 3.98 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 9.685773026493425\n",
            "FPR@TPR95: 33.60334926495853\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0406 seconds\n",
            "Total inference time (model only): 4.06 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 8.955769325574495\n",
            "FPR@TPR95: 37.006800856932955\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0397 seconds\n",
            "Total inference time (model only): 3.97 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 3.725060428820029\n",
            "FPR@TPR95: 42.27653056799948\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0393 seconds\n",
            "Total inference time (model only): 3.93 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 1.35657365622566\n",
            "FPR@TPR95: 70.64832708436916\n",
            "\n",
            "\n",
            "\n",
            "Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "\n",
            "--> Pruning: 0.1\n",
            "=======================================\n",
            "Avg inference time per image: 0.0399 seconds\n",
            "Total inference time (model only): 2.39 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 13.710427011795215\n",
            "FPR@TPR95: 82.94903364525844\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.2\n",
            "=======================================\n",
            "Avg inference time per image: 0.0396 seconds\n",
            "Total inference time (model only): 2.38 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 13.341944788885629\n",
            "FPR@TPR95: 83.05567372325288\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.3\n",
            "=======================================\n",
            "Avg inference time per image: 0.0395 seconds\n",
            "Total inference time (model only): 2.37 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 12.847447859275393\n",
            "FPR@TPR95: 86.0584865219241\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.4\n",
            "=======================================\n",
            "Avg inference time per image: 0.0395 seconds\n",
            "Total inference time (model only): 2.37 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 11.46096987080953\n",
            "FPR@TPR95: 88.7434666114507\n",
            "\n",
            "\n",
            "\n",
            "--> Pruning: 0.5\n",
            "=======================================\n",
            "Avg inference time per image: 0.0410 seconds\n",
            "Total inference time (model only): 2.46 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 10.078107214530574\n",
            "FPR@TPR95: 89.36574295573206\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization"
      ],
      "metadata": {
        "id": "U44GLzeg75Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ErfNet Quantization"
      ],
      "metadata": {
        "id": "6iko1fUJ54Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./eval/eval_iou.py \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\" \\\n",
        "    --loadWeights \"model_best.pth\" \\\n",
        "    --loadModel \"erfnet.py\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4 \\\n",
        "    --void \\\n",
        "    --cpu \\\n",
        "    --quantize"
      ],
      "metadata": {
        "id": "dp0NpfQS5vhP",
        "outputId": "39c394fc-8b62-4f43-d756-284ecb6508e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 2066836 | Non-zero (effective) parameters after pruning: 2066836\n",
            "Pruned percentage: 0.00%\n",
            "Unsupported operator aten::max_pool2d encountered 3 time(s)\n",
            "Unsupported operator aten::feature_dropout encountered 13 time(s)\n",
            "Unsupported operator aten::add encountered 17 time(s)\n",
            "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "decoder.layers.1.dropout, decoder.layers.2.dropout, decoder.layers.4.dropout, decoder.layers.5.dropout, encoder.output_conv\n",
            "Total FLOPs: 26.74 GFLOPs\n",
            "Estimated time: 0.041705 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Model quantized.\n",
            "  0% 0/500 [00:00<?, ?it/s]<eval_with_key>.5:11: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat = torch.cat([encoder_initial_block_conv, encoder_initial_block_pool], 1);  encoder_initial_block_conv = encoder_initial_block_pool = None\n",
            "<eval_with_key>.5:15: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_1 = torch.cat([encoder_layers_0_conv, encoder_layers_0_pool], 1);  encoder_layers_0_conv = encoder_layers_0_pool = None\n",
            "<eval_with_key>.5:79: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_2 = torch.cat([encoder_layers_6_conv, encoder_layers_6_pool], 1);  encoder_layers_6_conv = encoder_layers_6_pool = None\n",
            "100% 500/500 [14:51<00:00,  1.78s/it]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 41302.85it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 1.5288 seconds\n",
            "Total inference time (model only): 764.42 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m88.44\u001b[0m Road\n",
            "\u001b[0m53.76\u001b[0m sidewalk\n",
            "\u001b[0m69.58\u001b[0m building\n",
            "\u001b[0m7.03\u001b[0m wall\n",
            "\u001b[0m28.56\u001b[0m fence\n",
            "\u001b[0m40.31\u001b[0m pole\n",
            "\u001b[0m30.70\u001b[0m traffic light\n",
            "\u001b[0m49.29\u001b[0m traffic sign\n",
            "\u001b[0m70.41\u001b[0m vegetation\n",
            "\u001b[0m28.94\u001b[0m terrain\n",
            "\u001b[0m86.81\u001b[0m sky\n",
            "\u001b[0m47.83\u001b[0m person\n",
            "\u001b[0m16.20\u001b[0m rider\n",
            "\u001b[0m75.21\u001b[0m car\n",
            "\u001b[0m17.19\u001b[0m truck\n",
            "\u001b[0m21.65\u001b[0m bus\n",
            "\u001b[0m3.48\u001b[0m train\n",
            "\u001b[0m7.99\u001b[0m motorcycle\n",
            "\u001b[0m31.57\u001b[0m bicycle\n",
            "\u001b[0m52.98\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m41.40\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\n Evaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"/content/drive/MyDrive/training_results/erfnet_dec_40/\"\\\n",
        "            --loadWeights 'model_best.pth' \\\n",
        "            --loadModel \"erfnet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 2 \\\n",
        "            --method \"{method}\" \\\n",
        "            --cpu \\\n",
        "            --quantize\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "6Nogldyi6AAL",
        "outputId": "73374ddb-ce0d-4843-ec7d-073e6df0241f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "<eval_with_key>.5:11: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat = torch.cat([encoder_initial_block_conv, encoder_initial_block_pool], 1);  encoder_initial_block_conv = encoder_initial_block_pool = None\n",
            "<eval_with_key>.5:15: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_1 = torch.cat([encoder_layers_0_conv, encoder_layers_0_pool], 1);  encoder_layers_0_conv = encoder_layers_0_pool = None\n",
            "<eval_with_key>.5:79: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_2 = torch.cat([encoder_layers_6_conv, encoder_layers_6_pool], 1);  encoder_layers_6_conv = encoder_layers_6_pool = None\n",
            "=======================================\n",
            "Avg inference time per image: 1.9799 seconds\n",
            "Total inference time (model only): 19.80 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 21.62136988760166\n",
            "FPR@TPR95: 86.24816843040894\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "<eval_with_key>.5:11: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat = torch.cat([encoder_initial_block_conv, encoder_initial_block_pool], 1);  encoder_initial_block_conv = encoder_initial_block_pool = None\n",
            "<eval_with_key>.5:15: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_1 = torch.cat([encoder_layers_0_conv, encoder_layers_0_pool], 1);  encoder_layers_0_conv = encoder_layers_0_pool = None\n",
            "<eval_with_key>.5:79: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_2 = torch.cat([encoder_layers_6_conv, encoder_layers_6_pool], 1);  encoder_layers_6_conv = encoder_layers_6_pool = None\n",
            "=======================================\n",
            "Avg inference time per image: 1.9676 seconds\n",
            "Total inference time (model only): 59.03 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 0.7688222382024127\n",
            "FPR@TPR95: 80.93666574477669\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "<eval_with_key>.5:11: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat = torch.cat([encoder_initial_block_conv, encoder_initial_block_pool], 1);  encoder_initial_block_conv = encoder_initial_block_pool = None\n",
            "<eval_with_key>.5:15: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_1 = torch.cat([encoder_layers_0_conv, encoder_layers_0_pool], 1);  encoder_layers_0_conv = encoder_layers_0_pool = None\n",
            "<eval_with_key>.5:79: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_2 = torch.cat([encoder_layers_6_conv, encoder_layers_6_pool], 1);  encoder_layers_6_conv = encoder_layers_6_pool = None\n",
            "=======================================\n",
            "Avg inference time per image: 2.0122 seconds\n",
            "Total inference time (model only): 60.37 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 14.061096722642464\n",
            "FPR@TPR95: 35.879297147132945\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "<eval_with_key>.5:11: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat = torch.cat([encoder_initial_block_conv, encoder_initial_block_pool], 1);  encoder_initial_block_conv = encoder_initial_block_pool = None\n",
            "<eval_with_key>.5:15: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_1 = torch.cat([encoder_layers_0_conv, encoder_layers_0_pool], 1);  encoder_layers_0_conv = encoder_layers_0_pool = None\n",
            "<eval_with_key>.5:79: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_2 = torch.cat([encoder_layers_6_conv, encoder_layers_6_pool], 1);  encoder_layers_6_conv = encoder_layers_6_pool = None\n",
            "=======================================\n",
            "Avg inference time per image: 2.0830 seconds\n",
            "Total inference time (model only): 208.30 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 1.5742957879315993\n",
            "FPR@TPR95: 67.91552006283428\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "<eval_with_key>.5:11: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat = torch.cat([encoder_initial_block_conv, encoder_initial_block_pool], 1);  encoder_initial_block_conv = encoder_initial_block_pool = None\n",
            "<eval_with_key>.5:15: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_1 = torch.cat([encoder_layers_0_conv, encoder_layers_0_pool], 1);  encoder_layers_0_conv = encoder_layers_0_pool = None\n",
            "<eval_with_key>.5:79: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
            "  cat_2 = torch.cat([encoder_layers_6_conv, encoder_layers_6_pool], 1);  encoder_layers_6_conv = encoder_layers_6_pool = None\n",
            "=======================================\n",
            "Avg inference time per image: 2.0366 seconds\n",
            "Total inference time (model only): 122.20 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 10.828128967586112\n",
            "FPR@TPR95: 86.35684883047831\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiseNet Quantization"
      ],
      "metadata": {
        "id": "wsCq5M5p6YXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./eval/eval_iou.py \\\n",
        "    --loadDir \"/content/drive/MyDrive/training_results/bisenet/\" \\\n",
        "    --loadWeights \"model_best.pth\" \\\n",
        "    --loadModel \"./train/bisenet.py\" \\\n",
        "    --subset val \\\n",
        "    --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "    --batch-size 1 \\\n",
        "    --num-workers 4 \\\n",
        "    --void \\\n",
        "    --cpu \\\n",
        "    --quantize"
      ],
      "metadata": {
        "id": "CBprefAc6azZ",
        "outputId": "8996d003-eceb-4c02-b47a-d19622510e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 13426108 | Non-zero (effective) parameters after pruning: 13426108\n",
            "Pruned percentage: 0.00%\n",
            "Unsupported operator aten::max_pool2d encountered 1 time(s)\n",
            "Unsupported operator aten::add encountered 11 time(s)\n",
            "Unsupported operator aten::mean encountered 4 time(s)\n",
            "Unsupported operator aten::sigmoid encountered 3 time(s)\n",
            "Unsupported operator aten::mul encountered 3 time(s)\n",
            "Total FLOPs: 30.57 GFLOPs\n",
            "Estimated time: 0.047679 seconds\n",
            "/content/drive/MyDrive/cityscapes/leftImg8bit/val /content/drive/MyDrive/cityscapes/gtFine/val\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Preparing FX Graph Mode quantization...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "Calibrating model...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Model quantized.\n",
            "100% 500/500 [08:38<00:00,  1.04s/it]\n",
            "Processing IoU classes: 100% 20/20 [00:00<00:00, 45664.71it/s]\n",
            "=======================================\n",
            "Avg inference time per image: 0.7022 seconds\n",
            "Total inference time (model only): 351.10 seconds for 500 images\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.84\u001b[0m Road\n",
            "\u001b[0m67.04\u001b[0m sidewalk\n",
            "\u001b[0m84.16\u001b[0m building\n",
            "\u001b[0m38.57\u001b[0m wall\n",
            "\u001b[0m40.77\u001b[0m fence\n",
            "\u001b[0m40.69\u001b[0m pole\n",
            "\u001b[0m46.59\u001b[0m traffic light\n",
            "\u001b[0m54.70\u001b[0m traffic sign\n",
            "\u001b[0m86.84\u001b[0m vegetation\n",
            "\u001b[0m51.70\u001b[0m terrain\n",
            "\u001b[0m86.14\u001b[0m sky\n",
            "\u001b[0m63.16\u001b[0m person\n",
            "\u001b[0m41.87\u001b[0m rider\n",
            "\u001b[0m88.24\u001b[0m car\n",
            "\u001b[0m55.64\u001b[0m truck\n",
            "\u001b[0m67.39\u001b[0m bus\n",
            "\u001b[0m49.15\u001b[0m train\n",
            "\u001b[0m31.24\u001b[0m motorcycle\n",
            "\u001b[0m60.64\u001b[0m bicycle\n",
            "\u001b[0m67.25\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.73\u001b[0m %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Void\"]\n",
        "\n",
        "for dataset_name, input_path in input_paths.items():\n",
        "    print(f\"\\n Evaluating Dataset: {dataset_name}\")\n",
        "    print(\"=====================================\")\n",
        "    for method in methods:\n",
        "        !python3 eval/evalAnomaly.py \\\n",
        "            --input \"{input_path}\" \\\n",
        "            --loadDir \"/content/drive/MyDrive/training_results/bisenet/\"\\\n",
        "            --loadWeights 'model_best.pth' \\\n",
        "            --loadModel \"./train/bisenet.py\" \\\n",
        "            --subset val \\\n",
        "            --datadir \"/content/drive/MyDrive/cityscapes\" \\\n",
        "            --batch-size 1 \\\n",
        "            --num-workers 2 \\\n",
        "            --method \"{method}\" \\\n",
        "            --cpu \\\n",
        "            --quantize\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "eY-SMlHZ6fBr",
        "outputId": "61dab685-78d6-45a8-9d88-29009b3e1f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Evaluating Dataset: RoadAnomaly21\n",
            "=====================================\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 347MB/s]\n",
            "Preparing FX Graph Mode quantization...\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "=======================================\n",
            "Avg inference time per image: 0.7221 seconds\n",
            "Total inference time (model only): 7.22 seconds for 10 images\n",
            "=======================================\n",
            "AUPRC score: 31.652426262733\n",
            "FPR@TPR95: 73.51385103844807\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadObsticle21\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "=======================================\n",
            "Avg inference time per image: 0.7116 seconds\n",
            "Total inference time (model only): 21.35 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 8.057540014806339\n",
            "FPR@TPR95: 31.785061904635725\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: fs_static\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "=======================================\n",
            "Avg inference time per image: 0.7013 seconds\n",
            "Total inference time (model only): 21.04 seconds for 30 images\n",
            "=======================================\n",
            "AUPRC score: 4.986003314632777\n",
            "FPR@TPR95: 54.645796920607914\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: FS_LostFound_full\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "=======================================\n",
            "Avg inference time per image: 0.7189 seconds\n",
            "Total inference time (model only): 71.89 seconds for 100 images\n",
            "=======================================\n",
            "AUPRC score: 8.449822883544567\n",
            "FPR@TPR95: 42.537528216985706\n",
            "\n",
            "\n",
            "\n",
            " Evaluating Dataset: RoadAnomaly\n",
            "=====================================\n",
            "Preparing FX Graph Mode quantization...\n",
            "Calibrating model...\n",
            "Model quantized.\n",
            "=======================================\n",
            "Avg inference time per image: 0.7256 seconds\n",
            "Total inference time (model only): 43.54 seconds for 60 images\n",
            "=======================================\n",
            "AUPRC score: 15.924852247151811\n",
            "FPR@TPR95: 78.33104081731676\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}